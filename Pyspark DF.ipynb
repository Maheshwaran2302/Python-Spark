{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Spark\\\\spark-3.0.1-bin-hadoop2.7'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.csv(\"D:\\Trim 4\\Data Visualisation\\P1-UK-Bank-Customers-Chapter 4.csv\",inferSchema= True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Customer ID: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Surname: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- JobClassification: string (nullable = true)\n",
      " |-- DateJoined: string (nullable = true)\n",
      " |-- Balance: double (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "df = df.withColumn(\"Date\",df.Date.cast(DateType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Customer ID', 'int'),\n",
       " ('Name', 'string'),\n",
       " ('Surname', 'string'),\n",
       " ('Gender', 'string'),\n",
       " ('Age', 'int'),\n",
       " ('Region', 'string'),\n",
       " ('JobClassification', 'string'),\n",
       " ('DateJoined', 'string'),\n",
       " ('Balance', 'double'),\n",
       " ('Date', 'date')]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+---------+------+---+----------------+-----------------+----------+---------+----------+\n",
      "|Customer ID|     Name|  Surname|Gender|Age|          Region|JobClassification|DateJoined|  Balance|      Date|\n",
      "+-----------+---------+---------+------+---+----------------+-----------------+----------+---------+----------+\n",
      "|  100000001|    Simon|    Walsh|  Male| 21|         England|     White Collar| 05.Jan.15|113810.15|2021-01-01|\n",
      "|  400000002|  Jasmine|   Miller|Female| 34|Northern Ireland|      Blue Collar| 06.Jan.15| 36919.73|2021-01-01|\n",
      "|  100000003|     Liam|    Brown|  Male| 46|         England|     White Collar| 07.Jan.15|101536.83|2021-01-01|\n",
      "|  300000004|   Trevor|     Parr|  Male| 32|           Wales|     White Collar| 08.Jan.15|  1421.52|2021-01-01|\n",
      "|  100000005|  Deirdre|  Pullman|Female| 38|         England|      Blue Collar| 09.Jan.15| 35639.79|2021-01-01|\n",
      "|  300000006|      Ava|  Coleman|Female| 30|           Wales|      Blue Collar| 09.Jan.15|122443.77|2021-01-01|\n",
      "|  100000007|  Dorothy|  Thomson|Female| 34|         England|      Blue Collar| 11.Jan.15| 42879.84|2021-01-01|\n",
      "|  200000008|     Lisa|     Knox|Female| 48|        Scotland|            Other| 11.Jan.15| 36680.17|2021-01-01|\n",
      "|  300000009|     Ruth| Campbell|Female| 33|           Wales|     White Collar| 11.Jan.15| 74284.35|2021-01-01|\n",
      "|  100000010|  Dominic|     Parr|  Male| 42|         England|     White Collar| 12.Jan.15| 10912.45|2021-01-01|\n",
      "|  100000011|  Dominic|    Lewis|  Male| 40|         England|     White Collar| 12.Jan.15| 39667.83|2021-01-01|\n",
      "|  100000012| Benjamin|    Grant|  Male| 39|         England|     White Collar| 12.Jan.15| 32281.62|2021-01-01|\n",
      "|  100000013|     Ryan|MacDonald|  Male| 24|         England|     White Collar| 12.Jan.15| 40781.63|2021-01-01|\n",
      "|  200000014|   Thomas| Lawrence|  Male| 46|        Scotland|            Other| 12.Jan.15| 48791.46|2021-01-01|\n",
      "|  300000015|Madeleine| Marshall|Female| 36|           Wales|            Other| 12.Jan.15|  2846.03|2021-01-01|\n",
      "|  100000016| Nicholas|   Newman|  Male| 42|         England|     White Collar| 14.Jan.15|  2116.85|2021-01-01|\n",
      "|  200000017|    Grace|     Hill|Female| 31|        Scotland|            Other| 14.Jan.15| 10356.31|2021-01-01|\n",
      "|  200000018| Samantha|  Coleman|Female| 42|        Scotland|            Other| 14.Jan.15|  3801.69|2021-01-01|\n",
      "|  100000019|  William|     Ince|  Male| 40|         England|      Blue Collar| 15.Jan.15| 65534.69|2021-01-01|\n",
      "|  100000020|   Audrey|    Jones|Female| 46|         England|      Blue Collar| 15.Jan.15| 11462.64|2021-01-01|\n",
      "+-----------+---------+---------+------+---+----------------+-----------------+----------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Customer ID',\n",
       " 'Name',\n",
       " 'Surname',\n",
       " 'Gender',\n",
       " 'Age',\n",
       " 'Region',\n",
       " 'JobClassification',\n",
       " 'DateJoined',\n",
       " 'Balance',\n",
       " 'Date']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4014"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|               Age|\n",
      "+-------+------------------+\n",
      "|  count|              4014|\n",
      "|   mean|38.611111111111114|\n",
      "| stddev| 9.819120984901543|\n",
      "|    min|                15|\n",
      "|    max|                64|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe('Age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+---------+\n",
      "|     Name|Age|  Balance|\n",
      "+---------+---+---------+\n",
      "|    Simon| 21|113810.15|\n",
      "|  Jasmine| 34| 36919.73|\n",
      "|     Liam| 46|101536.83|\n",
      "|   Trevor| 32|  1421.52|\n",
      "|  Deirdre| 38| 35639.79|\n",
      "|      Ava| 30|122443.77|\n",
      "|  Dorothy| 34| 42879.84|\n",
      "|     Lisa| 48| 36680.17|\n",
      "|     Ruth| 33| 74284.35|\n",
      "|  Dominic| 42| 10912.45|\n",
      "|  Dominic| 40| 39667.83|\n",
      "| Benjamin| 39| 32281.62|\n",
      "|     Ryan| 24| 40781.63|\n",
      "|   Thomas| 46| 48791.46|\n",
      "|Madeleine| 36|  2846.03|\n",
      "| Nicholas| 42|  2116.85|\n",
      "|    Grace| 31| 10356.31|\n",
      "| Samantha| 42|  3801.69|\n",
      "|  William| 40| 65534.69|\n",
      "|   Audrey| 46| 11462.64|\n",
      "+---------+---+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('Name','Age','Balance').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|          Region|\n",
      "+----------------+\n",
      "|           Wales|\n",
      "|         England|\n",
      "|        Scotland|\n",
      "|Northern Ireland|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('Region').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|JobClassification|\n",
      "+-----------------+\n",
      "|      Blue Collar|\n",
      "|            Other|\n",
      "|     White Collar|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('JobClassification').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------------+\n",
      "|          Region|JobClassification|\n",
      "+----------------+-----------------+\n",
      "|         England|      Blue Collar|\n",
      "|        Scotland|      Blue Collar|\n",
      "|         England|     White Collar|\n",
      "|        Scotland|            Other|\n",
      "|           Wales|      Blue Collar|\n",
      "|         England|            Other|\n",
      "|Northern Ireland|            Other|\n",
      "|Northern Ireland|     White Collar|\n",
      "|           Wales|            Other|\n",
      "|Northern Ireland|      Blue Collar|\n",
      "|           Wales|     White Collar|\n",
      "|        Scotland|     White Collar|\n",
      "+----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('Region','JobClassification').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+---------+------+---+-------+-----------------+----------+---------+----------+\n",
      "|Customer ID|    Name|  Surname|Gender|Age| Region|JobClassification|DateJoined|  Balance|      Date|\n",
      "+-----------+--------+---------+------+---+-------+-----------------+----------+---------+----------+\n",
      "|  100000001|   Simon|    Walsh|  Male| 21|England|     White Collar| 05.Jan.15|113810.15|2021-01-01|\n",
      "|  100000003|    Liam|    Brown|  Male| 46|England|     White Collar| 07.Jan.15|101536.83|2021-01-01|\n",
      "|  100000005| Deirdre|  Pullman|Female| 38|England|      Blue Collar| 09.Jan.15| 35639.79|2021-01-01|\n",
      "|  100000007| Dorothy|  Thomson|Female| 34|England|      Blue Collar| 11.Jan.15| 42879.84|2021-01-01|\n",
      "|  100000010| Dominic|     Parr|  Male| 42|England|     White Collar| 12.Jan.15| 10912.45|2021-01-01|\n",
      "|  100000011| Dominic|    Lewis|  Male| 40|England|     White Collar| 12.Jan.15| 39667.83|2021-01-01|\n",
      "|  100000012|Benjamin|    Grant|  Male| 39|England|     White Collar| 12.Jan.15| 32281.62|2021-01-01|\n",
      "|  100000013|    Ryan|MacDonald|  Male| 24|England|     White Collar| 12.Jan.15| 40781.63|2021-01-01|\n",
      "|  100000016|Nicholas|   Newman|  Male| 42|England|     White Collar| 14.Jan.15|  2116.85|2021-01-01|\n",
      "|  100000019| William|     Ince|  Male| 40|England|      Blue Collar| 15.Jan.15| 65534.69|2021-01-01|\n",
      "|  100000020|  Audrey|    Jones|Female| 46|England|      Blue Collar| 15.Jan.15| 11462.64|2021-01-01|\n",
      "|  100000025|Jennifer|   Hughes|Female| 38|England|     White Collar| 20.Jan.15| 20505.32|2021-01-01|\n",
      "|  100000028|  Alison|   Fisher|Female| 33|England|     White Collar| 25.Jan.15|  3428.56|2021-01-01|\n",
      "|  100000030|    Luke|  Wilkins|  Male| 42|England|     White Collar| 27.Jan.15| 10813.83|2021-01-01|\n",
      "|  100000031|   Megan|     Bell|Female| 25|England|     White Collar| 28.Jan.15| 38889.76|2021-01-01|\n",
      "|  100000034|    Jake|   Wilson|  Male| 26|England|            Other| 30.Jan.15|  5833.62|2021-01-01|\n",
      "|  100000035|   Karen|     Dyer|Female| 34|England|      Blue Collar| 31.Jan.15| 77809.59|2021-01-01|\n",
      "|  100000036|  Yvonne|  Thomson|Female| 36|England|     White Collar| 31.Jan.15| 31680.67|2021-01-01|\n",
      "|  100000037|  Sophie|     Gill|Female| 33|England|     White Collar| 31.Jan.15| 20504.37|2021-01-01|\n",
      "|  100000038|  Robert|  Cornish|  Male| 31|England|            Other| 31.Jan.15|  5688.36|2021-01-01|\n",
      "+-----------+--------+---------+------+---+-------+-----------------+----------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.Region =='England').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2159"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(df.Region =='England').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+------+---+----------------+-----------------+----------+-------+----------+\n",
      "|CustomerID|     Name|  Surname|Gender|Age|          Region|JobClassification|DateJoined|Balance|      Date|\n",
      "+----------+---------+---------+------+---+----------------+-----------------+----------+-------+----------+\n",
      "| 100001320|     Jane|     King|Female| 38|         England|     White Collar| 22.Jul.15|  11.52|2021-01-01|\n",
      "| 400000075|   Olivia|     Dowd|Female| 30|Northern Ireland|     White Collar| 12.Feb.15|  21.03|2021-01-01|\n",
      "| 200000775|   Warren|  Roberts|  Male| 37|        Scotland|      Blue Collar| 01.Jun.15|  69.01|2021-01-01|\n",
      "| 400002046|    Megan|     Hart|Female| 19|Northern Ireland|      Blue Collar| 13.Sep.15|  69.78|2021-01-01|\n",
      "| 300003468|  Stewart| Johnston|  Male| 41|           Wales|      Blue Collar| 30.Nov.15|  77.46|2021-01-01|\n",
      "| 300003497|  Rebecca|   Howard|Female| 30|           Wales|      Blue Collar| 02.Dec.15|  96.26|2021-01-01|\n",
      "| 100003885|  Abigail|    Mills|Female| 29|         England|     White Collar| 23.Dec.15|  98.68|2021-01-01|\n",
      "| 100001419|  Lillian|   Mathis|Female| 43|         England|     White Collar| 28.Jul.15| 114.69|2021-01-01|\n",
      "| 400002278|    Pippa|     Ball|Female| 33|Northern Ireland|            Other| 24.Sep.15| 133.75|2021-01-01|\n",
      "| 100002735|    Chloe|     Gill|Female| 27|         England|      Blue Collar| 22.Oct.15| 134.94|2021-01-01|\n",
      "| 100000297|   Sophie|  Thomson|Female| 33|         England|     White Collar| 23.Apr.15| 152.91|2021-01-01|\n",
      "| 100003882|   Lauren|   Slater|Female| 35|         England|      Blue Collar| 23.Dec.15| 168.18|2021-01-01|\n",
      "| 200003843|    Colin|Mackenzie|  Male| 39|        Scotland|      Blue Collar| 20.Dec.15| 177.28|2021-01-01|\n",
      "| 200004013|Christian|     Reid|  Male| 51|        Scotland|      Blue Collar| 30.Dec.15| 239.45|2021-01-01|\n",
      "| 200001282|     Matt|   Watson|  Male| 44|        Scotland|     White Collar| 20.Jul.15| 242.49|2021-01-01|\n",
      "| 100000080|    David|   Peters|  Male| 34|         England|     White Collar| 16.Mar.15| 245.46|2021-01-01|\n",
      "| 100003558|      Sam|    Poole|  Male| 34|         England|     White Collar| 05.Dec.15| 251.67|2021-01-01|\n",
      "| 100002750|     Carl|   Fisher|  Male| 38|         England|     White Collar| 23.Oct.15| 267.38|2021-01-01|\n",
      "| 100003397|    Bella|  Ellison|Female| 30|         England|     White Collar| 27.Nov.15| 279.01|2021-01-01|\n",
      "| 100000378|    Harry|  Manning|  Male| 18|         England|     White Collar| 06.May.15| 311.26|2021-01-01|\n",
      "+----------+---------+---------+------+---+----------------+-----------------+----------+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(df.Balance).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df.sample(False,0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3224"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on DataFrame in module pyspark.sql.dataframe object:\n",
      "\n",
      "class DataFrame(pyspark.sql.pandas.map_ops.PandasMapOpsMixin, pyspark.sql.pandas.conversion.PandasConversionMixin)\n",
      " |  DataFrame(jdf, sql_ctx)\n",
      " |  \n",
      " |  A distributed collection of data grouped into named columns.\n",
      " |  \n",
      " |  A :class:`DataFrame` is equivalent to a relational table in Spark SQL,\n",
      " |  and can be created using various functions in :class:`SparkSession`::\n",
      " |  \n",
      " |      people = spark.read.parquet(\"...\")\n",
      " |  \n",
      " |  Once created, it can be manipulated using the various domain-specific-language\n",
      " |  (DSL) functions defined in: :class:`DataFrame`, :class:`Column`.\n",
      " |  \n",
      " |  To select a column from the :class:`DataFrame`, use the apply method::\n",
      " |  \n",
      " |      ageCol = people.age\n",
      " |  \n",
      " |  A more concrete example::\n",
      " |  \n",
      " |      # To create DataFrame using SparkSession\n",
      " |      people = spark.read.parquet(\"...\")\n",
      " |      department = spark.read.parquet(\"...\")\n",
      " |  \n",
      " |      people.filter(people.age > 30).join(department, people.deptId == department.id) \\\n",
      " |        .groupBy(department.name, \"gender\").agg({\"salary\": \"avg\", \"age\": \"max\"})\n",
      " |  \n",
      " |  .. versionadded:: 1.3\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      DataFrame\n",
      " |      pyspark.sql.pandas.map_ops.PandasMapOpsMixin\n",
      " |      pyspark.sql.pandas.conversion.PandasConversionMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getattr__(self, name)\n",
      " |      Returns the :class:`Column` denoted by ``name``.\n",
      " |      \n",
      " |      >>> df.select(df.age).collect()\n",
      " |      [Row(age=2), Row(age=5)]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  __getitem__(self, item)\n",
      " |      Returns the column as a :class:`Column`.\n",
      " |      \n",
      " |      >>> df.select(df['age']).collect()\n",
      " |      [Row(age=2), Row(age=5)]\n",
      " |      >>> df[ [\"name\", \"age\"]].collect()\n",
      " |      [Row(name='Alice', age=2), Row(name='Bob', age=5)]\n",
      " |      >>> df[ df.age > 3 ].collect()\n",
      " |      [Row(age=5, name='Bob')]\n",
      " |      >>> df[df[0] > 3].collect()\n",
      " |      [Row(age=5, name='Bob')]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  __init__(self, jdf, sql_ctx)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  agg(self, *exprs)\n",
      " |      Aggregate on the entire :class:`DataFrame` without groups\n",
      " |      (shorthand for ``df.groupBy.agg()``).\n",
      " |      \n",
      " |      >>> df.agg({\"age\": \"max\"}).collect()\n",
      " |      [Row(max(age)=5)]\n",
      " |      >>> from pyspark.sql import functions as F\n",
      " |      >>> df.agg(F.min(df.age)).collect()\n",
      " |      [Row(min(age)=2)]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  alias(self, alias)\n",
      " |      Returns a new :class:`DataFrame` with an alias set.\n",
      " |      \n",
      " |      :param alias: string, an alias name to be set for the :class:`DataFrame`.\n",
      " |      \n",
      " |      >>> from pyspark.sql.functions import *\n",
      " |      >>> df_as1 = df.alias(\"df_as1\")\n",
      " |      >>> df_as2 = df.alias(\"df_as2\")\n",
      " |      >>> joined_df = df_as1.join(df_as2, col(\"df_as1.name\") == col(\"df_as2.name\"), 'inner')\n",
      " |      >>> joined_df.select(\"df_as1.name\", \"df_as2.name\", \"df_as2.age\")                 .sort(desc(\"df_as1.name\")).collect()\n",
      " |      [Row(name='Bob', name='Bob', age=5), Row(name='Alice', name='Alice', age=2)]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  approxQuantile(self, col, probabilities, relativeError)\n",
      " |      Calculates the approximate quantiles of numerical columns of a\n",
      " |      :class:`DataFrame`.\n",
      " |      \n",
      " |      The result of this algorithm has the following deterministic bound:\n",
      " |      If the :class:`DataFrame` has N elements and if we request the quantile at\n",
      " |      probability `p` up to error `err`, then the algorithm will return\n",
      " |      a sample `x` from the :class:`DataFrame` so that the *exact* rank of `x` is\n",
      " |      close to (p * N). More precisely,\n",
      " |      \n",
      " |        floor((p - err) * N) <= rank(x) <= ceil((p + err) * N).\n",
      " |      \n",
      " |      This method implements a variation of the Greenwald-Khanna\n",
      " |      algorithm (with some speed optimizations). The algorithm was first\n",
      " |      present in [[https://doi.org/10.1145/375663.375670\n",
      " |      Space-efficient Online Computation of Quantile Summaries]]\n",
      " |      by Greenwald and Khanna.\n",
      " |      \n",
      " |      Note that null values will be ignored in numerical columns before calculation.\n",
      " |      For columns only containing null values, an empty list is returned.\n",
      " |      \n",
      " |      :param col: str, list.\n",
      " |        Can be a single column name, or a list of names for multiple columns.\n",
      " |      :param probabilities: a list of quantile probabilities\n",
      " |        Each number must belong to [0, 1].\n",
      " |        For example 0 is the minimum, 0.5 is the median, 1 is the maximum.\n",
      " |      :param relativeError:  The relative target precision to achieve\n",
      " |        (>= 0). If set to zero, the exact quantiles are computed, which\n",
      " |        could be very expensive. Note that values greater than 1 are\n",
      " |        accepted but give the same result as 1.\n",
      " |      :return:  the approximate quantiles at the given probabilities. If\n",
      " |        the input `col` is a string, the output is a list of floats. If the\n",
      " |        input `col` is a list or tuple of strings, the output is also a\n",
      " |        list, but each element in it is a list of floats, i.e., the output\n",
      " |        is a list of list of floats.\n",
      " |      \n",
      " |      .. versionchanged:: 2.2\n",
      " |         Added support for multiple columns.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  cache(self)\n",
      " |      Persists the :class:`DataFrame` with the default storage level (`MEMORY_AND_DISK`).\n",
      " |      \n",
      " |      .. note:: The default storage level has changed to `MEMORY_AND_DISK` to match Scala in 2.0.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  checkpoint(self, eager=True)\n",
      " |      Returns a checkpointed version of this Dataset. Checkpointing can be used to truncate the\n",
      " |      logical plan of this :class:`DataFrame`, which is especially useful in iterative algorithms\n",
      " |      where the plan may grow exponentially. It will be saved to files inside the checkpoint\n",
      " |      directory set with :meth:`SparkContext.setCheckpointDir`.\n",
      " |      \n",
      " |      :param eager: Whether to checkpoint this :class:`DataFrame` immediately\n",
      " |      \n",
      " |      .. note:: Experimental\n",
      " |      \n",
      " |      .. versionadded:: 2.1\n",
      " |  \n",
      " |  coalesce(self, numPartitions)\n",
      " |      Returns a new :class:`DataFrame` that has exactly `numPartitions` partitions.\n",
      " |      \n",
      " |      :param numPartitions: int, to specify the target number of partitions\n",
      " |      \n",
      " |      Similar to coalesce defined on an :class:`RDD`, this operation results in a\n",
      " |      narrow dependency, e.g. if you go from 1000 partitions to 100 partitions,\n",
      " |      there will not be a shuffle, instead each of the 100 new partitions will\n",
      " |      claim 10 of the current partitions. If a larger number of partitions is requested,\n",
      " |      it will stay at the current number of partitions.\n",
      " |      \n",
      " |      However, if you're doing a drastic coalesce, e.g. to numPartitions = 1,\n",
      " |      this may result in your computation taking place on fewer nodes than\n",
      " |      you like (e.g. one node in the case of numPartitions = 1). To avoid this,\n",
      " |      you can call repartition(). This will add a shuffle step, but means the\n",
      " |      current upstream partitions will be executed in parallel (per whatever\n",
      " |      the current partitioning is).\n",
      " |      \n",
      " |      >>> df.coalesce(1).rdd.getNumPartitions()\n",
      " |      1\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  colRegex(self, colName)\n",
      " |      Selects column based on the column name specified as a regex and returns it\n",
      " |      as :class:`Column`.\n",
      " |      \n",
      " |      :param colName: string, column name specified as a regex.\n",
      " |      \n",
      " |      >>> df = spark.createDataFrame([(\"a\", 1), (\"b\", 2), (\"c\",  3)], [\"Col1\", \"Col2\"])\n",
      " |      >>> df.select(df.colRegex(\"`(Col1)?+.+`\")).show()\n",
      " |      +----+\n",
      " |      |Col2|\n",
      " |      +----+\n",
      " |      |   1|\n",
      " |      |   2|\n",
      " |      |   3|\n",
      " |      +----+\n",
      " |      \n",
      " |      .. versionadded:: 2.3\n",
      " |  \n",
      " |  collect(self)\n",
      " |      Returns all the records as a list of :class:`Row`.\n",
      " |      \n",
      " |      >>> df.collect()\n",
      " |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  corr(self, col1, col2, method=None)\n",
      " |      Calculates the correlation of two columns of a :class:`DataFrame` as a double value.\n",
      " |      Currently only supports the Pearson Correlation Coefficient.\n",
      " |      :func:`DataFrame.corr` and :func:`DataFrameStatFunctions.corr` are aliases of each other.\n",
      " |      \n",
      " |      :param col1: The name of the first column\n",
      " |      :param col2: The name of the second column\n",
      " |      :param method: The correlation method. Currently only supports \"pearson\"\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  count(self)\n",
      " |      Returns the number of rows in this :class:`DataFrame`.\n",
      " |      \n",
      " |      >>> df.count()\n",
      " |      2\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  cov(self, col1, col2)\n",
      " |      Calculate the sample covariance for the given columns, specified by their names, as a\n",
      " |      double value. :func:`DataFrame.cov` and :func:`DataFrameStatFunctions.cov` are aliases.\n",
      " |      \n",
      " |      :param col1: The name of the first column\n",
      " |      :param col2: The name of the second column\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  createGlobalTempView(self, name)\n",
      " |      Creates a global temporary view with this :class:`DataFrame`.\n",
      " |      \n",
      " |      The lifetime of this temporary view is tied to this Spark application.\n",
      " |      throws :class:`TempTableAlreadyExistsException`, if the view name already exists in the\n",
      " |      catalog.\n",
      " |      \n",
      " |      >>> df.createGlobalTempView(\"people\")\n",
      " |      >>> df2 = spark.sql(\"select * from global_temp.people\")\n",
      " |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
      " |      True\n",
      " |      >>> df.createGlobalTempView(\"people\")  # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      " |      Traceback (most recent call last):\n",
      " |      ...\n",
      " |      AnalysisException: u\"Temporary table 'people' already exists;\"\n",
      " |      >>> spark.catalog.dropGlobalTempView(\"people\")\n",
      " |      \n",
      " |      .. versionadded:: 2.1\n",
      " |  \n",
      " |  createOrReplaceGlobalTempView(self, name)\n",
      " |      Creates or replaces a global temporary view using the given name.\n",
      " |      \n",
      " |      The lifetime of this temporary view is tied to this Spark application.\n",
      " |      \n",
      " |      >>> df.createOrReplaceGlobalTempView(\"people\")\n",
      " |      >>> df2 = df.filter(df.age > 3)\n",
      " |      >>> df2.createOrReplaceGlobalTempView(\"people\")\n",
      " |      >>> df3 = spark.sql(\"select * from global_temp.people\")\n",
      " |      >>> sorted(df3.collect()) == sorted(df2.collect())\n",
      " |      True\n",
      " |      >>> spark.catalog.dropGlobalTempView(\"people\")\n",
      " |      \n",
      " |      .. versionadded:: 2.2\n",
      " |  \n",
      " |  createOrReplaceTempView(self, name)\n",
      " |      Creates or replaces a local temporary view with this :class:`DataFrame`.\n",
      " |      \n",
      " |      The lifetime of this temporary table is tied to the :class:`SparkSession`\n",
      " |      that was used to create this :class:`DataFrame`.\n",
      " |      \n",
      " |      >>> df.createOrReplaceTempView(\"people\")\n",
      " |      >>> df2 = df.filter(df.age > 3)\n",
      " |      >>> df2.createOrReplaceTempView(\"people\")\n",
      " |      >>> df3 = spark.sql(\"select * from people\")\n",
      " |      >>> sorted(df3.collect()) == sorted(df2.collect())\n",
      " |      True\n",
      " |      >>> spark.catalog.dropTempView(\"people\")\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  createTempView(self, name)\n",
      " |      Creates a local temporary view with this :class:`DataFrame`.\n",
      " |      \n",
      " |      The lifetime of this temporary table is tied to the :class:`SparkSession`\n",
      " |      that was used to create this :class:`DataFrame`.\n",
      " |      throws :class:`TempTableAlreadyExistsException`, if the view name already exists in the\n",
      " |      catalog.\n",
      " |      \n",
      " |      >>> df.createTempView(\"people\")\n",
      " |      >>> df2 = spark.sql(\"select * from people\")\n",
      " |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
      " |      True\n",
      " |      >>> df.createTempView(\"people\")  # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      " |      Traceback (most recent call last):\n",
      " |      ...\n",
      " |      AnalysisException: u\"Temporary table 'people' already exists;\"\n",
      " |      >>> spark.catalog.dropTempView(\"people\")\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  crossJoin(self, other)\n",
      " |      Returns the cartesian product with another :class:`DataFrame`.\n",
      " |      \n",
      " |      :param other: Right side of the cartesian product.\n",
      " |      \n",
      " |      >>> df.select(\"age\", \"name\").collect()\n",
      " |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      " |      >>> df2.select(\"name\", \"height\").collect()\n",
      " |      [Row(name='Tom', height=80), Row(name='Bob', height=85)]\n",
      " |      >>> df.crossJoin(df2.select(\"height\")).select(\"age\", \"name\", \"height\").collect()\n",
      " |      [Row(age=2, name='Alice', height=80), Row(age=2, name='Alice', height=85),\n",
      " |       Row(age=5, name='Bob', height=80), Row(age=5, name='Bob', height=85)]\n",
      " |      \n",
      " |      .. versionadded:: 2.1\n",
      " |  \n",
      " |  crosstab(self, col1, col2)\n",
      " |      Computes a pair-wise frequency table of the given columns. Also known as a contingency\n",
      " |      table. The number of distinct values for each column should be less than 1e4. At most 1e6\n",
      " |      non-zero pair frequencies will be returned.\n",
      " |      The first column of each row will be the distinct values of `col1` and the column names\n",
      " |      will be the distinct values of `col2`. The name of the first column will be `$col1_$col2`.\n",
      " |      Pairs that have no occurrences will have zero as their counts.\n",
      " |      :func:`DataFrame.crosstab` and :func:`DataFrameStatFunctions.crosstab` are aliases.\n",
      " |      \n",
      " |      :param col1: The name of the first column. Distinct items will make the first item of\n",
      " |          each row.\n",
      " |      :param col2: The name of the second column. Distinct items will make the column names\n",
      " |          of the :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  cube(self, *cols)\n",
      " |      Create a multi-dimensional cube for the current :class:`DataFrame` using\n",
      " |      the specified columns, so we can run aggregations on them.\n",
      " |      \n",
      " |      >>> df.cube(\"name\", df.age).count().orderBy(\"name\", \"age\").show()\n",
      " |      +-----+----+-----+\n",
      " |      | name| age|count|\n",
      " |      +-----+----+-----+\n",
      " |      | null|null|    2|\n",
      " |      | null|   2|    1|\n",
      " |      | null|   5|    1|\n",
      " |      |Alice|null|    1|\n",
      " |      |Alice|   2|    1|\n",
      " |      |  Bob|null|    1|\n",
      " |      |  Bob|   5|    1|\n",
      " |      +-----+----+-----+\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  describe(self, *cols)\n",
      " |      Computes basic statistics for numeric and string columns.\n",
      " |      \n",
      " |      This include count, mean, stddev, min, and max. If no columns are\n",
      " |      given, this function computes statistics for all numerical or string columns.\n",
      " |      \n",
      " |      .. note:: This function is meant for exploratory data analysis, as we make no\n",
      " |          guarantee about the backward compatibility of the schema of the resulting\n",
      " |          :class:`DataFrame`.\n",
      " |      \n",
      " |      >>> df.describe(['age']).show()\n",
      " |      +-------+------------------+\n",
      " |      |summary|               age|\n",
      " |      +-------+------------------+\n",
      " |      |  count|                 2|\n",
      " |      |   mean|               3.5|\n",
      " |      | stddev|2.1213203435596424|\n",
      " |      |    min|                 2|\n",
      " |      |    max|                 5|\n",
      " |      +-------+------------------+\n",
      " |      >>> df.describe().show()\n",
      " |      +-------+------------------+-----+\n",
      " |      |summary|               age| name|\n",
      " |      +-------+------------------+-----+\n",
      " |      |  count|                 2|    2|\n",
      " |      |   mean|               3.5| null|\n",
      " |      | stddev|2.1213203435596424| null|\n",
      " |      |    min|                 2|Alice|\n",
      " |      |    max|                 5|  Bob|\n",
      " |      +-------+------------------+-----+\n",
      " |      \n",
      " |      Use summary for expanded statistics and control over which statistics to compute.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.1\n",
      " |  \n",
      " |  distinct(self)\n",
      " |      Returns a new :class:`DataFrame` containing the distinct rows in this :class:`DataFrame`.\n",
      " |      \n",
      " |      >>> df.distinct().count()\n",
      " |      2\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  drop(self, *cols)\n",
      " |      Returns a new :class:`DataFrame` that drops the specified column.\n",
      " |      This is a no-op if schema doesn't contain the given column name(s).\n",
      " |      \n",
      " |      :param cols: a string name of the column to drop, or a\n",
      " |          :class:`Column` to drop, or a list of string name of the columns to drop.\n",
      " |      \n",
      " |      >>> df.drop('age').collect()\n",
      " |      [Row(name='Alice'), Row(name='Bob')]\n",
      " |      \n",
      " |      >>> df.drop(df.age).collect()\n",
      " |      [Row(name='Alice'), Row(name='Bob')]\n",
      " |      \n",
      " |      >>> df.join(df2, df.name == df2.name, 'inner').drop(df.name).collect()\n",
      " |      [Row(age=5, height=85, name='Bob')]\n",
      " |      \n",
      " |      >>> df.join(df2, df.name == df2.name, 'inner').drop(df2.name).collect()\n",
      " |      [Row(age=5, name='Bob', height=85)]\n",
      " |      \n",
      " |      >>> df.join(df2, 'name', 'inner').drop('age', 'height').collect()\n",
      " |      [Row(name='Bob')]\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  dropDuplicates(self, subset=None)\n",
      " |      Return a new :class:`DataFrame` with duplicate rows removed,\n",
      " |      optionally only considering certain columns.\n",
      " |      \n",
      " |      For a static batch :class:`DataFrame`, it just drops duplicate rows. For a streaming\n",
      " |      :class:`DataFrame`, it will keep all data across triggers as intermediate state to drop\n",
      " |      duplicates rows. You can use :func:`withWatermark` to limit how late the duplicate data can\n",
      " |      be and system will accordingly limit the state. In addition, too late data older than\n",
      " |      watermark will be dropped to avoid any possibility of duplicates.\n",
      " |      \n",
      " |      :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n",
      " |      \n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = sc.parallelize([ \\\n",
      " |      ...     Row(name='Alice', age=5, height=80), \\\n",
      " |      ...     Row(name='Alice', age=5, height=80), \\\n",
      " |      ...     Row(name='Alice', age=10, height=80)]).toDF()\n",
      " |      >>> df.dropDuplicates().show()\n",
      " |      +---+------+-----+\n",
      " |      |age|height| name|\n",
      " |      +---+------+-----+\n",
      " |      |  5|    80|Alice|\n",
      " |      | 10|    80|Alice|\n",
      " |      +---+------+-----+\n",
      " |      \n",
      " |      >>> df.dropDuplicates(['name', 'height']).show()\n",
      " |      +---+------+-----+\n",
      " |      |age|height| name|\n",
      " |      +---+------+-----+\n",
      " |      |  5|    80|Alice|\n",
      " |      +---+------+-----+\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  drop_duplicates = dropDuplicates(self, subset=None)\n",
      " |      :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  dropna(self, how='any', thresh=None, subset=None)\n",
      " |      Returns a new :class:`DataFrame` omitting rows with null values.\n",
      " |      :func:`DataFrame.dropna` and :func:`DataFrameNaFunctions.drop` are aliases of each other.\n",
      " |      \n",
      " |      :param how: 'any' or 'all'.\n",
      " |          If 'any', drop a row if it contains any nulls.\n",
      " |          If 'all', drop a row only if all its values are null.\n",
      " |      :param thresh: int, default None\n",
      " |          If specified, drop rows that have less than `thresh` non-null values.\n",
      " |          This overwrites the `how` parameter.\n",
      " |      :param subset: optional list of column names to consider.\n",
      " |      \n",
      " |      >>> df4.na.drop().show()\n",
      " |      +---+------+-----+\n",
      " |      |age|height| name|\n",
      " |      +---+------+-----+\n",
      " |      | 10|    80|Alice|\n",
      " |      +---+------+-----+\n",
      " |      \n",
      " |      .. versionadded:: 1.3.1\n",
      " |  \n",
      " |  exceptAll(self, other)\n",
      " |      Return a new :class:`DataFrame` containing rows in this :class:`DataFrame` but\n",
      " |      not in another :class:`DataFrame` while preserving duplicates.\n",
      " |      \n",
      " |      This is equivalent to `EXCEPT ALL` in SQL.\n",
      " |      \n",
      " |      >>> df1 = spark.createDataFrame(\n",
      " |      ...         [(\"a\", 1), (\"a\", 1), (\"a\", 1), (\"a\", 2), (\"b\",  3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
      " |      >>> df2 = spark.createDataFrame([(\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n",
      " |      \n",
      " |      >>> df1.exceptAll(df2).show()\n",
      " |      +---+---+\n",
      " |      | C1| C2|\n",
      " |      +---+---+\n",
      " |      |  a|  1|\n",
      " |      |  a|  1|\n",
      " |      |  a|  2|\n",
      " |      |  c|  4|\n",
      " |      +---+---+\n",
      " |      \n",
      " |      Also as standard in SQL, this function resolves columns by position (not by name).\n",
      " |      \n",
      " |      .. versionadded:: 2.4\n",
      " |  \n",
      " |  explain(self, extended=None, mode=None)\n",
      " |      Prints the (logical and physical) plans to the console for debugging purpose.\n",
      " |      \n",
      " |      :param extended: boolean, default ``False``. If ``False``, prints only the physical plan.\n",
      " |          When this is a string without specifying the ``mode``, it works as the mode is\n",
      " |          specified.\n",
      " |      :param mode: specifies the expected output format of plans.\n",
      " |      \n",
      " |          * ``simple``: Print only a physical plan.\n",
      " |          * ``extended``: Print both logical and physical plans.\n",
      " |          * ``codegen``: Print a physical plan and generated codes if they are available.\n",
      " |          * ``cost``: Print a logical plan and statistics if they are available.\n",
      " |          * ``formatted``: Split explain output into two sections: a physical plan outline                 and node details.\n",
      " |      \n",
      " |      >>> df.explain()\n",
      " |      == Physical Plan ==\n",
      " |      *(1) Scan ExistingRDD[age#0,name#1]\n",
      " |      \n",
      " |      >>> df.explain(True)\n",
      " |      == Parsed Logical Plan ==\n",
      " |      ...\n",
      " |      == Analyzed Logical Plan ==\n",
      " |      ...\n",
      " |      == Optimized Logical Plan ==\n",
      " |      ...\n",
      " |      == Physical Plan ==\n",
      " |      ...\n",
      " |      \n",
      " |      >>> df.explain(mode=\"formatted\")\n",
      " |      == Physical Plan ==\n",
      " |      * Scan ExistingRDD (1)\n",
      " |      (1) Scan ExistingRDD [codegen id : 1]\n",
      " |      Output [2]: [age#0, name#1]\n",
      " |      ...\n",
      " |      \n",
      " |      >>> df.explain(\"cost\")\n",
      " |      == Optimized Logical Plan ==\n",
      " |      ...Statistics...\n",
      " |      ...\n",
      " |      \n",
      " |      .. versionchanged:: 3.0.0\n",
      " |         Added optional argument `mode` to specify the expected output format of plans.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  fillna(self, value, subset=None)\n",
      " |      Replace null values, alias for ``na.fill()``.\n",
      " |      :func:`DataFrame.fillna` and :func:`DataFrameNaFunctions.fill` are aliases of each other.\n",
      " |      \n",
      " |      :param value: int, long, float, string, bool or dict.\n",
      " |          Value to replace null values with.\n",
      " |          If the value is a dict, then `subset` is ignored and `value` must be a mapping\n",
      " |          from column name (string) to replacement value. The replacement value must be\n",
      " |          an int, long, float, boolean, or string.\n",
      " |      :param subset: optional list of column names to consider.\n",
      " |          Columns specified in subset that do not have matching data type are ignored.\n",
      " |          For example, if `value` is a string, and subset contains a non-string column,\n",
      " |          then the non-string column is simply ignored.\n",
      " |      \n",
      " |      >>> df4.na.fill(50).show()\n",
      " |      +---+------+-----+\n",
      " |      |age|height| name|\n",
      " |      +---+------+-----+\n",
      " |      | 10|    80|Alice|\n",
      " |      |  5|    50|  Bob|\n",
      " |      | 50|    50|  Tom|\n",
      " |      | 50|    50| null|\n",
      " |      +---+------+-----+\n",
      " |      \n",
      " |      >>> df5.na.fill(False).show()\n",
      " |      +----+-------+-----+\n",
      " |      | age|   name|  spy|\n",
      " |      +----+-------+-----+\n",
      " |      |  10|  Alice|false|\n",
      " |      |   5|    Bob|false|\n",
      " |      |null|Mallory| true|\n",
      " |      +----+-------+-----+\n",
      " |      \n",
      " |      >>> df4.na.fill({'age': 50, 'name': 'unknown'}).show()\n",
      " |      +---+------+-------+\n",
      " |      |age|height|   name|\n",
      " |      +---+------+-------+\n",
      " |      | 10|    80|  Alice|\n",
      " |      |  5|  null|    Bob|\n",
      " |      | 50|  null|    Tom|\n",
      " |      | 50|  null|unknown|\n",
      " |      +---+------+-------+\n",
      " |      \n",
      " |      .. versionadded:: 1.3.1\n",
      " |  \n",
      " |  filter(self, condition)\n",
      " |      Filters rows using the given condition.\n",
      " |      \n",
      " |      :func:`where` is an alias for :func:`filter`.\n",
      " |      \n",
      " |      :param condition: a :class:`Column` of :class:`types.BooleanType`\n",
      " |          or a string of SQL expression.\n",
      " |      \n",
      " |      >>> df.filter(df.age > 3).collect()\n",
      " |      [Row(age=5, name='Bob')]\n",
      " |      >>> df.where(df.age == 2).collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |      \n",
      " |      >>> df.filter(\"age > 3\").collect()\n",
      " |      [Row(age=5, name='Bob')]\n",
      " |      >>> df.where(\"age = 2\").collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  first(self)\n",
      " |      Returns the first row as a :class:`Row`.\n",
      " |      \n",
      " |      >>> df.first()\n",
      " |      Row(age=2, name='Alice')\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  foreach(self, f)\n",
      " |      Applies the ``f`` function to all :class:`Row` of this :class:`DataFrame`.\n",
      " |      \n",
      " |      This is a shorthand for ``df.rdd.foreach()``.\n",
      " |      \n",
      " |      >>> def f(person):\n",
      " |      ...     print(person.name)\n",
      " |      >>> df.foreach(f)\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  foreachPartition(self, f)\n",
      " |      Applies the ``f`` function to each partition of this :class:`DataFrame`.\n",
      " |      \n",
      " |      This a shorthand for ``df.rdd.foreachPartition()``.\n",
      " |      \n",
      " |      >>> def f(people):\n",
      " |      ...     for person in people:\n",
      " |      ...         print(person.name)\n",
      " |      >>> df.foreachPartition(f)\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  freqItems(self, cols, support=None)\n",
      " |      Finding frequent items for columns, possibly with false positives. Using the\n",
      " |      frequent element count algorithm described in\n",
      " |      \"https://doi.org/10.1145/762471.762473, proposed by Karp, Schenker, and Papadimitriou\".\n",
      " |      :func:`DataFrame.freqItems` and :func:`DataFrameStatFunctions.freqItems` are aliases.\n",
      " |      \n",
      " |      .. note:: This function is meant for exploratory data analysis, as we make no\n",
      " |          guarantee about the backward compatibility of the schema of the resulting\n",
      " |          :class:`DataFrame`.\n",
      " |      \n",
      " |      :param cols: Names of the columns to calculate frequent items for as a list or tuple of\n",
      " |          strings.\n",
      " |      :param support: The frequency with which to consider an item 'frequent'. Default is 1%.\n",
      " |          The support must be greater than 1e-4.\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  groupBy(self, *cols)\n",
      " |      Groups the :class:`DataFrame` using the specified columns,\n",
      " |      so we can run aggregation on them. See :class:`GroupedData`\n",
      " |      for all the available aggregate functions.\n",
      " |      \n",
      " |      :func:`groupby` is an alias for :func:`groupBy`.\n",
      " |      \n",
      " |      :param cols: list of columns to group by.\n",
      " |          Each element should be a column name (string) or an expression (:class:`Column`).\n",
      " |      \n",
      " |      >>> df.groupBy().avg().collect()\n",
      " |      [Row(avg(age)=3.5)]\n",
      " |      >>> sorted(df.groupBy('name').agg({'age': 'mean'}).collect())\n",
      " |      [Row(name='Alice', avg(age)=2.0), Row(name='Bob', avg(age)=5.0)]\n",
      " |      >>> sorted(df.groupBy(df.name).avg().collect())\n",
      " |      [Row(name='Alice', avg(age)=2.0), Row(name='Bob', avg(age)=5.0)]\n",
      " |      >>> sorted(df.groupBy(['name', df.age]).count().collect())\n",
      " |      [Row(name='Alice', age=2, count=1), Row(name='Bob', age=5, count=1)]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  groupby = groupBy(self, *cols)\n",
      " |      :func:`groupby` is an alias for :func:`groupBy`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  head(self, n=None)\n",
      " |      Returns the first ``n`` rows.\n",
      " |      \n",
      " |      .. note:: This method should only be used if the resulting array is expected\n",
      " |          to be small, as all the data is loaded into the driver's memory.\n",
      " |      \n",
      " |      :param n: int, default 1. Number of rows to return.\n",
      " |      :return: If n is greater than 1, return a list of :class:`Row`.\n",
      " |          If n is 1, return a single Row.\n",
      " |      \n",
      " |      >>> df.head()\n",
      " |      Row(age=2, name='Alice')\n",
      " |      >>> df.head(1)\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  hint(self, name, *parameters)\n",
      " |      Specifies some hint on the current :class:`DataFrame`.\n",
      " |      \n",
      " |      :param name: A name of the hint.\n",
      " |      :param parameters: Optional parameters.\n",
      " |      :return: :class:`DataFrame`\n",
      " |      \n",
      " |      >>> df.join(df2.hint(\"broadcast\"), \"name\").show()\n",
      " |      +----+---+------+\n",
      " |      |name|age|height|\n",
      " |      +----+---+------+\n",
      " |      | Bob|  5|    85|\n",
      " |      +----+---+------+\n",
      " |      \n",
      " |      .. versionadded:: 2.2\n",
      " |  \n",
      " |  intersect(self, other)\n",
      " |      Return a new :class:`DataFrame` containing rows only in\n",
      " |      both this :class:`DataFrame` and another :class:`DataFrame`.\n",
      " |      \n",
      " |      This is equivalent to `INTERSECT` in SQL.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  intersectAll(self, other)\n",
      " |      Return a new :class:`DataFrame` containing rows in both this :class:`DataFrame`\n",
      " |      and another :class:`DataFrame` while preserving duplicates.\n",
      " |      \n",
      " |      This is equivalent to `INTERSECT ALL` in SQL.\n",
      " |      >>> df1 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
      " |      >>> df2 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n",
      " |      \n",
      " |      >>> df1.intersectAll(df2).sort(\"C1\", \"C2\").show()\n",
      " |      +---+---+\n",
      " |      | C1| C2|\n",
      " |      +---+---+\n",
      " |      |  a|  1|\n",
      " |      |  a|  1|\n",
      " |      |  b|  3|\n",
      " |      +---+---+\n",
      " |      \n",
      " |      Also as standard in SQL, this function resolves columns by position (not by name).\n",
      " |      \n",
      " |      .. versionadded:: 2.4\n",
      " |  \n",
      " |  isLocal(self)\n",
      " |      Returns ``True`` if the :func:`collect` and :func:`take` methods can be run locally\n",
      " |      (without any Spark executors).\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  join(self, other, on=None, how=None)\n",
      " |      Joins with another :class:`DataFrame`, using the given join expression.\n",
      " |      \n",
      " |      :param other: Right side of the join\n",
      " |      :param on: a string for the join column name, a list of column names,\n",
      " |          a join expression (Column), or a list of Columns.\n",
      " |          If `on` is a string or a list of strings indicating the name of the join column(s),\n",
      " |          the column(s) must exist on both sides, and this performs an equi-join.\n",
      " |      :param how: str, default ``inner``. Must be one of: ``inner``, ``cross``, ``outer``,\n",
      " |          ``full``, ``fullouter``, ``full_outer``, ``left``, ``leftouter``, ``left_outer``,\n",
      " |          ``right``, ``rightouter``, ``right_outer``, ``semi``, ``leftsemi``, ``left_semi``,\n",
      " |          ``anti``, ``leftanti`` and ``left_anti``.\n",
      " |      \n",
      " |      The following performs a full outer join between ``df1`` and ``df2``.\n",
      " |      >>> from pyspark.sql.functions import desc\n",
      " |      >>> df.join(df2, df.name == df2.name, 'outer').select(df.name, df2.height)                 .sort(desc(\"name\")).collect()\n",
      " |      [Row(name='Bob', height=85), Row(name='Alice', height=None), Row(name=None, height=80)]\n",
      " |      \n",
      " |      >>> df.join(df2, 'name', 'outer').select('name', 'height').sort(desc(\"name\")).collect()\n",
      " |      [Row(name='Tom', height=80), Row(name='Bob', height=85), Row(name='Alice', height=None)]\n",
      " |      \n",
      " |      >>> cond = [df.name == df3.name, df.age == df3.age]\n",
      " |      >>> df.join(df3, cond, 'outer').select(df.name, df3.age).collect()\n",
      " |      [Row(name='Alice', age=2), Row(name='Bob', age=5)]\n",
      " |      \n",
      " |      >>> df.join(df2, 'name').select(df.name, df2.height).collect()\n",
      " |      [Row(name='Bob', height=85)]\n",
      " |      \n",
      " |      >>> df.join(df4, ['name', 'age']).select(df.name, df.age).collect()\n",
      " |      [Row(name='Bob', age=5)]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  limit(self, num)\n",
      " |      Limits the result count to the number specified.\n",
      " |      \n",
      " |      >>> df.limit(1).collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |      >>> df.limit(0).collect()\n",
      " |      []\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  localCheckpoint(self, eager=True)\n",
      " |      Returns a locally checkpointed version of this Dataset. Checkpointing can be used to\n",
      " |      truncate the logical plan of this :class:`DataFrame`, which is especially useful in\n",
      " |      iterative algorithms where the plan may grow exponentially. Local checkpoints are\n",
      " |      stored in the executors using the caching subsystem and therefore they are not reliable.\n",
      " |      \n",
      " |      :param eager: Whether to checkpoint this :class:`DataFrame` immediately\n",
      " |      \n",
      " |      .. note:: Experimental\n",
      " |      \n",
      " |      .. versionadded:: 2.3\n",
      " |  \n",
      " |  orderBy = sort(self, *cols, **kwargs)\n",
      " |  \n",
      " |  persist(self, storageLevel=StorageLevel(True, True, False, False, 1))\n",
      " |      Sets the storage level to persist the contents of the :class:`DataFrame` across\n",
      " |      operations after the first time it is computed. This can only be used to assign\n",
      " |      a new storage level if the :class:`DataFrame` does not have a storage level set yet.\n",
      " |      If no storage level is specified defaults to (`MEMORY_AND_DISK`).\n",
      " |      \n",
      " |      .. note:: The default storage level has changed to `MEMORY_AND_DISK` to match Scala in 2.0.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  printSchema(self)\n",
      " |      Prints out the schema in the tree format.\n",
      " |      \n",
      " |      >>> df.printSchema()\n",
      " |      root\n",
      " |       |-- age: integer (nullable = true)\n",
      " |       |-- name: string (nullable = true)\n",
      " |      <BLANKLINE>\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  randomSplit(self, weights, seed=None)\n",
      " |      Randomly splits this :class:`DataFrame` with the provided weights.\n",
      " |      \n",
      " |      :param weights: list of doubles as weights with which to split the :class:`DataFrame`.\n",
      " |          Weights will be normalized if they don't sum up to 1.0.\n",
      " |      :param seed: The seed for sampling.\n",
      " |      \n",
      " |      >>> splits = df4.randomSplit([1.0, 2.0], 24)\n",
      " |      >>> splits[0].count()\n",
      " |      2\n",
      " |      \n",
      " |      >>> splits[1].count()\n",
      " |      2\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  registerTempTable(self, name)\n",
      " |      Registers this DataFrame as a temporary table using the given name.\n",
      " |      \n",
      " |      The lifetime of this temporary table is tied to the :class:`SparkSession`\n",
      " |      that was used to create this :class:`DataFrame`.\n",
      " |      \n",
      " |      >>> df.registerTempTable(\"people\")\n",
      " |      >>> df2 = spark.sql(\"select * from people\")\n",
      " |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
      " |      True\n",
      " |      >>> spark.catalog.dropTempView(\"people\")\n",
      " |      \n",
      " |      .. note:: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  repartition(self, numPartitions, *cols)\n",
      " |      Returns a new :class:`DataFrame` partitioned by the given partitioning expressions. The\n",
      " |      resulting :class:`DataFrame` is hash partitioned.\n",
      " |      \n",
      " |      :param numPartitions:\n",
      " |          can be an int to specify the target number of partitions or a Column.\n",
      " |          If it is a Column, it will be used as the first partitioning column. If not specified,\n",
      " |          the default number of partitions is used.\n",
      " |      \n",
      " |      .. versionchanged:: 1.6\n",
      " |         Added optional arguments to specify the partitioning columns. Also made numPartitions\n",
      " |         optional if partitioning columns are specified.\n",
      " |      \n",
      " |      >>> df.repartition(10).rdd.getNumPartitions()\n",
      " |      10\n",
      " |      >>> data = df.union(df).repartition(\"age\")\n",
      " |      >>> data.show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  5|  Bob|\n",
      " |      |  5|  Bob|\n",
      " |      |  2|Alice|\n",
      " |      |  2|Alice|\n",
      " |      +---+-----+\n",
      " |      >>> data = data.repartition(7, \"age\")\n",
      " |      >>> data.show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  2|Alice|\n",
      " |      |  5|  Bob|\n",
      " |      |  2|Alice|\n",
      " |      |  5|  Bob|\n",
      " |      +---+-----+\n",
      " |      >>> data.rdd.getNumPartitions()\n",
      " |      7\n",
      " |      >>> data = data.repartition(\"name\", \"age\")\n",
      " |      >>> data.show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  5|  Bob|\n",
      " |      |  5|  Bob|\n",
      " |      |  2|Alice|\n",
      " |      |  2|Alice|\n",
      " |      +---+-----+\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  repartitionByRange(self, numPartitions, *cols)\n",
      " |      Returns a new :class:`DataFrame` partitioned by the given partitioning expressions. The\n",
      " |      resulting :class:`DataFrame` is range partitioned.\n",
      " |      \n",
      " |      :param numPartitions:\n",
      " |          can be an int to specify the target number of partitions or a Column.\n",
      " |          If it is a Column, it will be used as the first partitioning column. If not specified,\n",
      " |          the default number of partitions is used.\n",
      " |      \n",
      " |      At least one partition-by expression must be specified.\n",
      " |      When no explicit sort order is specified, \"ascending nulls first\" is assumed.\n",
      " |      \n",
      " |      Note that due to performance reasons this method uses sampling to estimate the ranges.\n",
      " |      Hence, the output may not be consistent, since sampling can return different values.\n",
      " |      The sample size can be controlled by the config\n",
      " |      `spark.sql.execution.rangeExchange.sampleSizePerPartition`.\n",
      " |      \n",
      " |      >>> df.repartitionByRange(2, \"age\").rdd.getNumPartitions()\n",
      " |      2\n",
      " |      >>> df.show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  2|Alice|\n",
      " |      |  5|  Bob|\n",
      " |      +---+-----+\n",
      " |      >>> df.repartitionByRange(1, \"age\").rdd.getNumPartitions()\n",
      " |      1\n",
      " |      >>> data = df.repartitionByRange(\"age\")\n",
      " |      >>> df.show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  2|Alice|\n",
      " |      |  5|  Bob|\n",
      " |      +---+-----+\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |  \n",
      " |  replace(self, to_replace, value=<no value>, subset=None)\n",
      " |      Returns a new :class:`DataFrame` replacing a value with another value.\n",
      " |      :func:`DataFrame.replace` and :func:`DataFrameNaFunctions.replace` are\n",
      " |      aliases of each other.\n",
      " |      Values to_replace and value must have the same type and can only be numerics, booleans,\n",
      " |      or strings. Value can have None. When replacing, the new value will be cast\n",
      " |      to the type of the existing column.\n",
      " |      For numeric replacements all values to be replaced should have unique\n",
      " |      floating point representation. In case of conflicts (for example with `{42: -1, 42.0: 1}`)\n",
      " |      and arbitrary replacement will be used.\n",
      " |      \n",
      " |      :param to_replace: bool, int, long, float, string, list or dict.\n",
      " |          Value to be replaced.\n",
      " |          If the value is a dict, then `value` is ignored or can be omitted, and `to_replace`\n",
      " |          must be a mapping between a value and a replacement.\n",
      " |      :param value: bool, int, long, float, string, list or None.\n",
      " |          The replacement value must be a bool, int, long, float, string or None. If `value` is a\n",
      " |          list, `value` should be of the same length and type as `to_replace`.\n",
      " |          If `value` is a scalar and `to_replace` is a sequence, then `value` is\n",
      " |          used as a replacement for each item in `to_replace`.\n",
      " |      :param subset: optional list of column names to consider.\n",
      " |          Columns specified in subset that do not have matching data type are ignored.\n",
      " |          For example, if `value` is a string, and subset contains a non-string column,\n",
      " |          then the non-string column is simply ignored.\n",
      " |      \n",
      " |      >>> df4.na.replace(10, 20).show()\n",
      " |      +----+------+-----+\n",
      " |      | age|height| name|\n",
      " |      +----+------+-----+\n",
      " |      |  20|    80|Alice|\n",
      " |      |   5|  null|  Bob|\n",
      " |      |null|  null|  Tom|\n",
      " |      |null|  null| null|\n",
      " |      +----+------+-----+\n",
      " |      \n",
      " |      >>> df4.na.replace('Alice', None).show()\n",
      " |      +----+------+----+\n",
      " |      | age|height|name|\n",
      " |      +----+------+----+\n",
      " |      |  10|    80|null|\n",
      " |      |   5|  null| Bob|\n",
      " |      |null|  null| Tom|\n",
      " |      |null|  null|null|\n",
      " |      +----+------+----+\n",
      " |      \n",
      " |      >>> df4.na.replace({'Alice': None}).show()\n",
      " |      +----+------+----+\n",
      " |      | age|height|name|\n",
      " |      +----+------+----+\n",
      " |      |  10|    80|null|\n",
      " |      |   5|  null| Bob|\n",
      " |      |null|  null| Tom|\n",
      " |      |null|  null|null|\n",
      " |      +----+------+----+\n",
      " |      \n",
      " |      >>> df4.na.replace(['Alice', 'Bob'], ['A', 'B'], 'name').show()\n",
      " |      +----+------+----+\n",
      " |      | age|height|name|\n",
      " |      +----+------+----+\n",
      " |      |  10|    80|   A|\n",
      " |      |   5|  null|   B|\n",
      " |      |null|  null| Tom|\n",
      " |      |null|  null|null|\n",
      " |      +----+------+----+\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  rollup(self, *cols)\n",
      " |      Create a multi-dimensional rollup for the current :class:`DataFrame` using\n",
      " |      the specified columns, so we can run aggregation on them.\n",
      " |      \n",
      " |      >>> df.rollup(\"name\", df.age).count().orderBy(\"name\", \"age\").show()\n",
      " |      +-----+----+-----+\n",
      " |      | name| age|count|\n",
      " |      +-----+----+-----+\n",
      " |      | null|null|    2|\n",
      " |      |Alice|null|    1|\n",
      " |      |Alice|   2|    1|\n",
      " |      |  Bob|null|    1|\n",
      " |      |  Bob|   5|    1|\n",
      " |      +-----+----+-----+\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  sample(self, withReplacement=None, fraction=None, seed=None)\n",
      " |      Returns a sampled subset of this :class:`DataFrame`.\n",
      " |      \n",
      " |      :param withReplacement: Sample with replacement or not (default ``False``).\n",
      " |      :param fraction: Fraction of rows to generate, range [0.0, 1.0].\n",
      " |      :param seed: Seed for sampling (default a random seed).\n",
      " |      \n",
      " |      .. note:: This is not guaranteed to provide exactly the fraction specified of the total\n",
      " |          count of the given :class:`DataFrame`.\n",
      " |      \n",
      " |      .. note:: `fraction` is required and, `withReplacement` and `seed` are optional.\n",
      " |      \n",
      " |      >>> df = spark.range(10)\n",
      " |      >>> df.sample(0.5, 3).count()\n",
      " |      7\n",
      " |      >>> df.sample(fraction=0.5, seed=3).count()\n",
      " |      7\n",
      " |      >>> df.sample(withReplacement=True, fraction=0.5, seed=3).count()\n",
      " |      1\n",
      " |      >>> df.sample(1.0).count()\n",
      " |      10\n",
      " |      >>> df.sample(fraction=1.0).count()\n",
      " |      10\n",
      " |      >>> df.sample(False, fraction=1.0).count()\n",
      " |      10\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  sampleBy(self, col, fractions, seed=None)\n",
      " |      Returns a stratified sample without replacement based on the\n",
      " |      fraction given on each stratum.\n",
      " |      \n",
      " |      :param col: column that defines strata\n",
      " |      :param fractions:\n",
      " |          sampling fraction for each stratum. If a stratum is not\n",
      " |          specified, we treat its fraction as zero.\n",
      " |      :param seed: random seed\n",
      " |      :return: a new :class:`DataFrame` that represents the stratified sample\n",
      " |      \n",
      " |      >>> from pyspark.sql.functions import col\n",
      " |      >>> dataset = sqlContext.range(0, 100).select((col(\"id\") % 3).alias(\"key\"))\n",
      " |      >>> sampled = dataset.sampleBy(\"key\", fractions={0: 0.1, 1: 0.2}, seed=0)\n",
      " |      >>> sampled.groupBy(\"key\").count().orderBy(\"key\").show()\n",
      " |      +---+-----+\n",
      " |      |key|count|\n",
      " |      +---+-----+\n",
      " |      |  0|    3|\n",
      " |      |  1|    6|\n",
      " |      +---+-----+\n",
      " |      >>> dataset.sampleBy(col(\"key\"), fractions={2: 1.0}, seed=0).count()\n",
      " |      33\n",
      " |      \n",
      " |      .. versionchanged:: 3.0\n",
      " |         Added sampling by a column of :class:`Column`\n",
      " |      \n",
      " |      .. versionadded:: 1.5\n",
      " |  \n",
      " |  select(self, *cols)\n",
      " |      Projects a set of expressions and returns a new :class:`DataFrame`.\n",
      " |      \n",
      " |      :param cols: list of column names (string) or expressions (:class:`Column`).\n",
      " |          If one of the column names is '*', that column is expanded to include all columns\n",
      " |          in the current :class:`DataFrame`.\n",
      " |      \n",
      " |      >>> df.select('*').collect()\n",
      " |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      " |      >>> df.select('name', 'age').collect()\n",
      " |      [Row(name='Alice', age=2), Row(name='Bob', age=5)]\n",
      " |      >>> df.select(df.name, (df.age + 10).alias('age')).collect()\n",
      " |      [Row(name='Alice', age=12), Row(name='Bob', age=15)]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  selectExpr(self, *expr)\n",
      " |      Projects a set of SQL expressions and returns a new :class:`DataFrame`.\n",
      " |      \n",
      " |      This is a variant of :func:`select` that accepts SQL expressions.\n",
      " |      \n",
      " |      >>> df.selectExpr(\"age * 2\", \"abs(age)\").collect()\n",
      " |      [Row((age * 2)=4, abs(age)=2), Row((age * 2)=10, abs(age)=5)]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  show(self, n=20, truncate=True, vertical=False)\n",
      " |      Prints the first ``n`` rows to the console.\n",
      " |      \n",
      " |      :param n: Number of rows to show.\n",
      " |      :param truncate: If set to ``True``, truncate strings longer than 20 chars by default.\n",
      " |          If set to a number greater than one, truncates long strings to length ``truncate``\n",
      " |          and align cells right.\n",
      " |      :param vertical: If set to ``True``, print output rows vertically (one line\n",
      " |          per column value).\n",
      " |      \n",
      " |      >>> df\n",
      " |      DataFrame[age: int, name: string]\n",
      " |      >>> df.show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  2|Alice|\n",
      " |      |  5|  Bob|\n",
      " |      +---+-----+\n",
      " |      >>> df.show(truncate=3)\n",
      " |      +---+----+\n",
      " |      |age|name|\n",
      " |      +---+----+\n",
      " |      |  2| Ali|\n",
      " |      |  5| Bob|\n",
      " |      +---+----+\n",
      " |      >>> df.show(vertical=True)\n",
      " |      -RECORD 0-----\n",
      " |       age  | 2\n",
      " |       name | Alice\n",
      " |      -RECORD 1-----\n",
      " |       age  | 5\n",
      " |       name | Bob\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  sort(self, *cols, **kwargs)\n",
      " |      Returns a new :class:`DataFrame` sorted by the specified column(s).\n",
      " |      \n",
      " |      :param cols: list of :class:`Column` or column names to sort by.\n",
      " |      :param ascending: boolean or list of boolean (default ``True``).\n",
      " |          Sort ascending vs. descending. Specify list for multiple sort orders.\n",
      " |          If a list is specified, length of the list must equal length of the `cols`.\n",
      " |      \n",
      " |      >>> df.sort(df.age.desc()).collect()\n",
      " |      [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      " |      >>> df.sort(\"age\", ascending=False).collect()\n",
      " |      [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      " |      >>> df.orderBy(df.age.desc()).collect()\n",
      " |      [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      " |      >>> from pyspark.sql.functions import *\n",
      " |      >>> df.sort(asc(\"age\")).collect()\n",
      " |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      " |      >>> df.orderBy(desc(\"age\"), \"name\").collect()\n",
      " |      [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      " |      >>> df.orderBy([\"age\", \"name\"], ascending=[0, 1]).collect()\n",
      " |      [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  sortWithinPartitions(self, *cols, **kwargs)\n",
      " |      Returns a new :class:`DataFrame` with each partition sorted by the specified column(s).\n",
      " |      \n",
      " |      :param cols: list of :class:`Column` or column names to sort by.\n",
      " |      :param ascending: boolean or list of boolean (default ``True``).\n",
      " |          Sort ascending vs. descending. Specify list for multiple sort orders.\n",
      " |          If a list is specified, length of the list must equal length of the `cols`.\n",
      " |      \n",
      " |      >>> df.sortWithinPartitions(\"age\", ascending=False).show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  2|Alice|\n",
      " |      |  5|  Bob|\n",
      " |      +---+-----+\n",
      " |      \n",
      " |      .. versionadded:: 1.6\n",
      " |  \n",
      " |  subtract(self, other)\n",
      " |      Return a new :class:`DataFrame` containing rows in this :class:`DataFrame`\n",
      " |      but not in another :class:`DataFrame`.\n",
      " |      \n",
      " |      This is equivalent to `EXCEPT DISTINCT` in SQL.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  summary(self, *statistics)\n",
      " |      Computes specified statistics for numeric and string columns. Available statistics are:\n",
      " |      - count\n",
      " |      - mean\n",
      " |      - stddev\n",
      " |      - min\n",
      " |      - max\n",
      " |      - arbitrary approximate percentiles specified as a percentage (eg, 75%)\n",
      " |      \n",
      " |      If no statistics are given, this function computes count, mean, stddev, min,\n",
      " |      approximate quartiles (percentiles at 25%, 50%, and 75%), and max.\n",
      " |      \n",
      " |      .. note:: This function is meant for exploratory data analysis, as we make no\n",
      " |          guarantee about the backward compatibility of the schema of the resulting\n",
      " |          :class:`DataFrame`.\n",
      " |      \n",
      " |      >>> df.summary().show()\n",
      " |      +-------+------------------+-----+\n",
      " |      |summary|               age| name|\n",
      " |      +-------+------------------+-----+\n",
      " |      |  count|                 2|    2|\n",
      " |      |   mean|               3.5| null|\n",
      " |      | stddev|2.1213203435596424| null|\n",
      " |      |    min|                 2|Alice|\n",
      " |      |    25%|                 2| null|\n",
      " |      |    50%|                 2| null|\n",
      " |      |    75%|                 5| null|\n",
      " |      |    max|                 5|  Bob|\n",
      " |      +-------+------------------+-----+\n",
      " |      \n",
      " |      >>> df.summary(\"count\", \"min\", \"25%\", \"75%\", \"max\").show()\n",
      " |      +-------+---+-----+\n",
      " |      |summary|age| name|\n",
      " |      +-------+---+-----+\n",
      " |      |  count|  2|    2|\n",
      " |      |    min|  2|Alice|\n",
      " |      |    25%|  2| null|\n",
      " |      |    75%|  5| null|\n",
      " |      |    max|  5|  Bob|\n",
      " |      +-------+---+-----+\n",
      " |      \n",
      " |      To do a summary for specific columns first select them:\n",
      " |      \n",
      " |      >>> df.select(\"age\", \"name\").summary(\"count\").show()\n",
      " |      +-------+---+----+\n",
      " |      |summary|age|name|\n",
      " |      +-------+---+----+\n",
      " |      |  count|  2|   2|\n",
      " |      +-------+---+----+\n",
      " |      \n",
      " |      See also describe for basic statistics.\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |  \n",
      " |  tail(self, num)\n",
      " |      Returns the last ``num`` rows as a :class:`list` of :class:`Row`.\n",
      " |      \n",
      " |      Running tail requires moving data into the application's driver process, and doing so with\n",
      " |      a very large ``num`` can crash the driver process with OutOfMemoryError.\n",
      " |      \n",
      " |      >>> df.tail(1)\n",
      " |      [Row(age=5, name='Bob')]\n",
      " |      \n",
      " |      .. versionadded:: 3.0\n",
      " |  \n",
      " |  take(self, num)\n",
      " |      Returns the first ``num`` rows as a :class:`list` of :class:`Row`.\n",
      " |      \n",
      " |      >>> df.take(2)\n",
      " |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  toDF(self, *cols)\n",
      " |      Returns a new :class:`DataFrame` that with new specified column names\n",
      " |      \n",
      " |      :param cols: list of new column names (string)\n",
      " |      \n",
      " |      >>> df.toDF('f1', 'f2').collect()\n",
      " |      [Row(f1=2, f2='Alice'), Row(f1=5, f2='Bob')]\n",
      " |  \n",
      " |  toJSON(self, use_unicode=True)\n",
      " |      Converts a :class:`DataFrame` into a :class:`RDD` of string.\n",
      " |      \n",
      " |      Each row is turned into a JSON document as one element in the returned RDD.\n",
      " |      \n",
      " |      >>> df.toJSON().first()\n",
      " |      '{\"age\":2,\"name\":\"Alice\"}'\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  toLocalIterator(self, prefetchPartitions=False)\n",
      " |      Returns an iterator that contains all of the rows in this :class:`DataFrame`.\n",
      " |      The iterator will consume as much memory as the largest partition in this\n",
      " |      :class:`DataFrame`. With prefetch it may consume up to the memory of the 2 largest\n",
      " |      partitions.\n",
      " |      \n",
      " |      :param prefetchPartitions: If Spark should pre-fetch the next partition\n",
      " |                                 before it is needed.\n",
      " |      \n",
      " |      >>> list(df.toLocalIterator())\n",
      " |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  transform(self, func)\n",
      " |      Returns a new :class:`DataFrame`. Concise syntax for chaining custom transformations.\n",
      " |      \n",
      " |      :param func: a function that takes and returns a :class:`DataFrame`.\n",
      " |      \n",
      " |      >>> from pyspark.sql.functions import col\n",
      " |      >>> df = spark.createDataFrame([(1, 1.0), (2, 2.0)], [\"int\", \"float\"])\n",
      " |      >>> def cast_all_to_int(input_df):\n",
      " |      ...     return input_df.select([col(col_name).cast(\"int\") for col_name in input_df.columns])\n",
      " |      >>> def sort_columns_asc(input_df):\n",
      " |      ...     return input_df.select(*sorted(input_df.columns))\n",
      " |      >>> df.transform(cast_all_to_int).transform(sort_columns_asc).show()\n",
      " |      +-----+---+\n",
      " |      |float|int|\n",
      " |      +-----+---+\n",
      " |      |    1|  1|\n",
      " |      |    2|  2|\n",
      " |      +-----+---+\n",
      " |      \n",
      " |      .. versionadded:: 3.0\n",
      " |  \n",
      " |  union(self, other)\n",
      " |      Return a new :class:`DataFrame` containing union of rows in this and another\n",
      " |      :class:`DataFrame`.\n",
      " |      \n",
      " |      This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union\n",
      " |      (that does deduplication of elements), use this function followed by :func:`distinct`.\n",
      " |      \n",
      " |      Also as standard in SQL, this function resolves columns by position (not by name).\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  unionAll(self, other)\n",
      " |      Return a new :class:`DataFrame` containing union of rows in this and another\n",
      " |      :class:`DataFrame`.\n",
      " |      \n",
      " |      This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union\n",
      " |      (that does deduplication of elements), use this function followed by :func:`distinct`.\n",
      " |      \n",
      " |      Also as standard in SQL, this function resolves columns by position (not by name).\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  unionByName(self, other)\n",
      " |      Returns a new :class:`DataFrame` containing union of rows in this and another\n",
      " |      :class:`DataFrame`.\n",
      " |      \n",
      " |      This is different from both `UNION ALL` and `UNION DISTINCT` in SQL. To do a SQL-style set\n",
      " |      union (that does deduplication of elements), use this function followed by :func:`distinct`.\n",
      " |      \n",
      " |      The difference between this function and :func:`union` is that this function\n",
      " |      resolves columns by name (not by position):\n",
      " |      \n",
      " |      >>> df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n",
      " |      >>> df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col0\"])\n",
      " |      >>> df1.unionByName(df2).show()\n",
      " |      +----+----+----+\n",
      " |      |col0|col1|col2|\n",
      " |      +----+----+----+\n",
      " |      |   1|   2|   3|\n",
      " |      |   6|   4|   5|\n",
      " |      +----+----+----+\n",
      " |      \n",
      " |      .. versionadded:: 2.3\n",
      " |  \n",
      " |  unpersist(self, blocking=False)\n",
      " |      Marks the :class:`DataFrame` as non-persistent, and remove all blocks for it from\n",
      " |      memory and disk.\n",
      " |      \n",
      " |      .. note:: `blocking` default has changed to ``False`` to match Scala in 2.0.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  where = filter(self, condition)\n",
      " |      :func:`where` is an alias for :func:`filter`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  withColumn(self, colName, col)\n",
      " |      Returns a new :class:`DataFrame` by adding a column or replacing the\n",
      " |      existing column that has the same name.\n",
      " |      \n",
      " |      The column expression must be an expression over this :class:`DataFrame`; attempting to add\n",
      " |      a column from some other :class:`DataFrame` will raise an error.\n",
      " |      \n",
      " |      :param colName: string, name of the new column.\n",
      " |      :param col: a :class:`Column` expression for the new column.\n",
      " |      \n",
      " |      .. note:: This method introduces a projection internally. Therefore, calling it multiple\n",
      " |          times, for instance, via loops in order to add multiple columns can generate big\n",
      " |          plans which can cause performance issues and even `StackOverflowException`.\n",
      " |          To avoid this, use :func:`select` with the multiple columns at once.\n",
      " |      \n",
      " |      >>> df.withColumn('age2', df.age + 2).collect()\n",
      " |      [Row(age=2, name='Alice', age2=4), Row(age=5, name='Bob', age2=7)]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  withColumnRenamed(self, existing, new)\n",
      " |      Returns a new :class:`DataFrame` by renaming an existing column.\n",
      " |      This is a no-op if schema doesn't contain the given column name.\n",
      " |      \n",
      " |      :param existing: string, name of the existing column to rename.\n",
      " |      :param new: string, new name of the column.\n",
      " |      \n",
      " |      >>> df.withColumnRenamed('age', 'age2').collect()\n",
      " |      [Row(age2=2, name='Alice'), Row(age2=5, name='Bob')]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  withWatermark(self, eventTime, delayThreshold)\n",
      " |      Defines an event time watermark for this :class:`DataFrame`. A watermark tracks a point\n",
      " |      in time before which we assume no more late data is going to arrive.\n",
      " |      \n",
      " |      Spark will use this watermark for several purposes:\n",
      " |        - To know when a given time window aggregation can be finalized and thus can be emitted\n",
      " |          when using output modes that do not allow updates.\n",
      " |      \n",
      " |        - To minimize the amount of state that we need to keep for on-going aggregations.\n",
      " |      \n",
      " |      The current watermark is computed by looking at the `MAX(eventTime)` seen across\n",
      " |      all of the partitions in the query minus a user specified `delayThreshold`.  Due to the cost\n",
      " |      of coordinating this value across partitions, the actual watermark used is only guaranteed\n",
      " |      to be at least `delayThreshold` behind the actual event time.  In some cases we may still\n",
      " |      process records that arrive more than `delayThreshold` late.\n",
      " |      \n",
      " |      :param eventTime: the name of the column that contains the event time of the row.\n",
      " |      :param delayThreshold: the minimum delay to wait to data to arrive late, relative to the\n",
      " |          latest record that has been processed in the form of an interval\n",
      " |          (e.g. \"1 minute\" or \"5 hours\").\n",
      " |      \n",
      " |      .. note:: Evolving\n",
      " |      \n",
      " |      >>> sdf.select('name', sdf.time.cast('timestamp')).withWatermark('time', '10 minutes')\n",
      " |      DataFrame[name: string, time: timestamp]\n",
      " |      \n",
      " |      .. versionadded:: 2.1\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  columns\n",
      " |      Returns all column names as a list.\n",
      " |      \n",
      " |      >>> df.columns\n",
      " |      ['age', 'name']\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  dtypes\n",
      " |      Returns all column names and their data types as a list.\n",
      " |      \n",
      " |      >>> df.dtypes\n",
      " |      [('age', 'int'), ('name', 'string')]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  isStreaming\n",
      " |      Returns ``True`` if this :class:`Dataset` contains one or more sources that continuously\n",
      " |      return data as it arrives. A :class:`Dataset` that reads data from a streaming source\n",
      " |      must be executed as a :class:`StreamingQuery` using the :func:`start` method in\n",
      " |      :class:`DataStreamWriter`.  Methods that return a single answer, (e.g., :func:`count` or\n",
      " |      :func:`collect`) will throw an :class:`AnalysisException` when there is a streaming\n",
      " |      source present.\n",
      " |      \n",
      " |      .. note:: Evolving\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  na\n",
      " |      Returns a :class:`DataFrameNaFunctions` for handling missing values.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.1\n",
      " |  \n",
      " |  rdd\n",
      " |      Returns the content as an :class:`pyspark.RDD` of :class:`Row`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  schema\n",
      " |      Returns the schema of this :class:`DataFrame` as a :class:`pyspark.sql.types.StructType`.\n",
      " |      \n",
      " |      >>> df.schema\n",
      " |      StructType(List(StructField(age,IntegerType,true),StructField(name,StringType,true)))\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  stat\n",
      " |      Returns a :class:`DataFrameStatFunctions` for statistic functions.\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  storageLevel\n",
      " |      Get the :class:`DataFrame`'s current storage level.\n",
      " |      \n",
      " |      >>> df.storageLevel\n",
      " |      StorageLevel(False, False, False, False, 1)\n",
      " |      >>> df.cache().storageLevel\n",
      " |      StorageLevel(True, True, False, True, 1)\n",
      " |      >>> df2.persist(StorageLevel.DISK_ONLY_2).storageLevel\n",
      " |      StorageLevel(True, False, False, False, 2)\n",
      " |      \n",
      " |      .. versionadded:: 2.1\n",
      " |  \n",
      " |  write\n",
      " |      Interface for saving the content of the non-streaming :class:`DataFrame` out into external\n",
      " |      storage.\n",
      " |      \n",
      " |      :return: :class:`DataFrameWriter`\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  writeStream\n",
      " |      Interface for saving the content of the streaming :class:`DataFrame` out into external\n",
      " |      storage.\n",
      " |      \n",
      " |      .. note:: Evolving.\n",
      " |      \n",
      " |      :return: :class:`DataStreamWriter`\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.sql.pandas.map_ops.PandasMapOpsMixin:\n",
      " |  \n",
      " |  mapInPandas(self, func, schema)\n",
      " |      Maps an iterator of batches in the current :class:`DataFrame` using a Python native\n",
      " |      function that takes and outputs a pandas DataFrame, and returns the result as a\n",
      " |      :class:`DataFrame`.\n",
      " |      \n",
      " |      The function should take an iterator of `pandas.DataFrame`\\s and return\n",
      " |      another iterator of `pandas.DataFrame`\\s. All columns are passed\n",
      " |      together as an iterator of `pandas.DataFrame`\\s to the function and the\n",
      " |      returned iterator of `pandas.DataFrame`\\s are combined as a :class:`DataFrame`.\n",
      " |      Each `pandas.DataFrame` size can be controlled by\n",
      " |      `spark.sql.execution.arrow.maxRecordsPerBatch`.\n",
      " |      \n",
      " |      :param func: a Python native function that takes an iterator of `pandas.DataFrame`\\s, and\n",
      " |          outputs an iterator of `pandas.DataFrame`\\s.\n",
      " |      :param schema: the return type of the `func` in PySpark. The value can be either a\n",
      " |          :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
      " |      \n",
      " |      >>> from pyspark.sql.functions import pandas_udf\n",
      " |      >>> df = spark.createDataFrame([(1, 21), (2, 30)], (\"id\", \"age\"))\n",
      " |      >>> def filter_func(iterator):\n",
      " |      ...     for pdf in iterator:\n",
      " |      ...         yield pdf[pdf.id == 1]\n",
      " |      >>> df.mapInPandas(filter_func, df.schema).show()  # doctest: +SKIP\n",
      " |      +---+---+\n",
      " |      | id|age|\n",
      " |      +---+---+\n",
      " |      |  1| 21|\n",
      " |      +---+---+\n",
      " |      \n",
      " |      .. seealso:: :meth:`pyspark.sql.functions.pandas_udf`\n",
      " |      \n",
      " |      .. note:: Experimental\n",
      " |      \n",
      " |      .. versionadded:: 3.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pyspark.sql.pandas.map_ops.PandasMapOpsMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.sql.pandas.conversion.PandasConversionMixin:\n",
      " |  \n",
      " |  toPandas(self)\n",
      " |      Returns the contents of this :class:`DataFrame` as Pandas ``pandas.DataFrame``.\n",
      " |      \n",
      " |      This is only available if Pandas is installed and available.\n",
      " |      \n",
      " |      .. note:: This method should only be used if the resulting Pandas's :class:`DataFrame` is\n",
      " |          expected to be small, as all the data is loaded into the driver's memory.\n",
      " |      \n",
      " |      .. note:: Usage with spark.sql.execution.arrow.pyspark.enabled=True is experimental.\n",
      " |      \n",
      " |      >>> df.toPandas()  # doctest: +SKIP\n",
      " |         age   name\n",
      " |      0    2  Alice\n",
      " |      1    5    Bob\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+---------+------+---+----------------+-----------------+----------+---------+----------+\n",
      "|Customer ID|     Name|  Surname|Gender|Age|          Region|JobClassification|DateJoined|  Balance|      Date|\n",
      "+-----------+---------+---------+------+---+----------------+-----------------+----------+---------+----------+\n",
      "|  100000001|    Simon|    Walsh|  Male| 21|         England|     White Collar| 05.Jan.15|113810.15|2021-01-01|\n",
      "|  400000002|  Jasmine|   Miller|Female| 34|Northern Ireland|      Blue Collar| 06.Jan.15| 36919.73|2021-01-01|\n",
      "|  100000003|     Liam|    Brown|  Male| 46|         England|     White Collar| 07.Jan.15|101536.83|2021-01-01|\n",
      "|  300000004|   Trevor|     Parr|  Male| 32|           Wales|     White Collar| 08.Jan.15|  1421.52|2021-01-01|\n",
      "|  100000005|  Deirdre|  Pullman|Female| 38|         England|      Blue Collar| 09.Jan.15| 35639.79|2021-01-01|\n",
      "|  300000006|      Ava|  Coleman|Female| 30|           Wales|      Blue Collar| 09.Jan.15|122443.77|2021-01-01|\n",
      "|  100000007|  Dorothy|  Thomson|Female| 34|         England|      Blue Collar| 11.Jan.15| 42879.84|2021-01-01|\n",
      "|  200000008|     Lisa|     Knox|Female| 48|        Scotland|            Other| 11.Jan.15| 36680.17|2021-01-01|\n",
      "|  300000009|     Ruth| Campbell|Female| 33|           Wales|     White Collar| 11.Jan.15| 74284.35|2021-01-01|\n",
      "|  100000010|  Dominic|     Parr|  Male| 42|         England|     White Collar| 12.Jan.15| 10912.45|2021-01-01|\n",
      "|  100000011|  Dominic|    Lewis|  Male| 40|         England|     White Collar| 12.Jan.15| 39667.83|2021-01-01|\n",
      "|  100000012| Benjamin|    Grant|  Male| 39|         England|     White Collar| 12.Jan.15| 32281.62|2021-01-01|\n",
      "|  100000013|     Ryan|MacDonald|  Male| 24|         England|     White Collar| 12.Jan.15| 40781.63|2021-01-01|\n",
      "|  200000014|   Thomas| Lawrence|  Male| 46|        Scotland|            Other| 12.Jan.15| 48791.46|2021-01-01|\n",
      "|  300000015|Madeleine| Marshall|Female| 36|           Wales|            Other| 12.Jan.15|  2846.03|2021-01-01|\n",
      "|  100000016| Nicholas|   Newman|  Male| 42|         England|     White Collar| 14.Jan.15|  2116.85|2021-01-01|\n",
      "|  200000017|    Grace|     Hill|Female| 31|        Scotland|            Other| 14.Jan.15| 10356.31|2021-01-01|\n",
      "|  200000018| Samantha|  Coleman|Female| 42|        Scotland|            Other| 14.Jan.15|  3801.69|2021-01-01|\n",
      "|  100000019|  William|     Ince|  Male| 40|         England|      Blue Collar| 15.Jan.15| 65534.69|2021-01-01|\n",
      "|  100000020|   Audrey|    Jones|Female| 46|         England|      Blue Collar| 15.Jan.15| 11462.64|2021-01-01|\n",
      "+-----------+---------+---------+------+---+----------------+-----------------+----------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('*').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.functions import date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o458.showString.\n: org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: Fail to recognize 'YYYYMM' pattern in the DateTimeFormatter. 1) You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\r\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkLegacyFormatter$1.applyOrElse(DateTimeFormatterHelper.scala:196)\r\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkLegacyFormatter$1.applyOrElse(DateTimeFormatterHelper.scala:185)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.validatePatternString(TimestampFormatter.scala:109)\r\n\tat org.apache.spark.sql.catalyst.util.TimestampFormatter$.getFormatter(TimestampFormatter.scala:278)\r\n\tat org.apache.spark.sql.catalyst.util.TimestampFormatter$.apply(TimestampFormatter.scala:312)\r\n\tat org.apache.spark.sql.catalyst.expressions.DateFormatClass.$anonfun$formatter$1(datetimeExpressions.scala:646)\r\n\tat scala.Option.map(Option.scala:230)\r\n\tat org.apache.spark.sql.catalyst.expressions.DateFormatClass.formatter$lzycompute(datetimeExpressions.scala:641)\r\n\tat org.apache.spark.sql.catalyst.expressions.DateFormatClass.formatter(datetimeExpressions.scala:639)\r\n\tat org.apache.spark.sql.catalyst.expressions.DateFormatClass.doGenCode(datetimeExpressions.scala:665)\r\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:146)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:141)\r\n\tat org.apache.spark.sql.catalyst.expressions.Alias.genCode(namedExpressions.scala:159)\r\n\tat org.apache.spark.sql.execution.ProjectExec.$anonfun$doConsume$1(basicPhysicalOperators.scala:66)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\r\n\tat org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:66)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:194)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:149)\r\n\tat org.apache.spark.sql.execution.InputAdapter.consume(WholeStageCodegenExec.scala:496)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce(WholeStageCodegenExec.scala:483)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce$(WholeStageCodegenExec.scala:456)\r\n\tat org.apache.spark.sql.execution.InputAdapter.doProduce(WholeStageCodegenExec.scala:496)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\r\n\tat org.apache.spark.sql.execution.InputAdapter.produce(WholeStageCodegenExec.scala:496)\r\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:51)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\r\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:41)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:632)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:692)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\r\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:316)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:434)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:420)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3627)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2697)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2697)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2904)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:337)\r\n\tat sun.reflect.GeneratedMethodAccessor80.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.lang.IllegalArgumentException: All week-based patterns are unsupported since Spark 3.0, detected: Y, Please use the SQL function EXTRACT instead\r\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.$anonfun$convertIncompatiblePattern$4(DateTimeFormatterHelper.scala:323)\r\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.$anonfun$convertIncompatiblePattern$4$adapted(DateTimeFormatterHelper.scala:321)\r\n\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.immutable.StringOps.foreach(StringOps.scala:33)\r\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)\r\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.$anonfun$convertIncompatiblePattern$2(DateTimeFormatterHelper.scala:321)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\r\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.convertIncompatiblePattern(DateTimeFormatterHelper.scala:318)\r\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper.getOrCreateFormatter(DateTimeFormatterHelper.scala:121)\r\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper.getOrCreateFormatter$(DateTimeFormatterHelper.scala:117)\r\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.getOrCreateFormatter(TimestampFormatter.scala:59)\r\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.formatter$lzycompute(TimestampFormatter.scala:68)\r\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.formatter(TimestampFormatter.scala:67)\r\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.validatePatternString(TimestampFormatter.scala:108)\r\n\t... 75 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-172-ff0c39629c4a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdate_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"YYYYMM\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    438\u001b[0m         \"\"\"\n\u001b[0;32m    439\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1305\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o458.showString.\n: org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: Fail to recognize 'YYYYMM' pattern in the DateTimeFormatter. 1) You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\r\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkLegacyFormatter$1.applyOrElse(DateTimeFormatterHelper.scala:196)\r\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkLegacyFormatter$1.applyOrElse(DateTimeFormatterHelper.scala:185)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.validatePatternString(TimestampFormatter.scala:109)\r\n\tat org.apache.spark.sql.catalyst.util.TimestampFormatter$.getFormatter(TimestampFormatter.scala:278)\r\n\tat org.apache.spark.sql.catalyst.util.TimestampFormatter$.apply(TimestampFormatter.scala:312)\r\n\tat org.apache.spark.sql.catalyst.expressions.DateFormatClass.$anonfun$formatter$1(datetimeExpressions.scala:646)\r\n\tat scala.Option.map(Option.scala:230)\r\n\tat org.apache.spark.sql.catalyst.expressions.DateFormatClass.formatter$lzycompute(datetimeExpressions.scala:641)\r\n\tat org.apache.spark.sql.catalyst.expressions.DateFormatClass.formatter(datetimeExpressions.scala:639)\r\n\tat org.apache.spark.sql.catalyst.expressions.DateFormatClass.doGenCode(datetimeExpressions.scala:665)\r\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:146)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:141)\r\n\tat org.apache.spark.sql.catalyst.expressions.Alias.genCode(namedExpressions.scala:159)\r\n\tat org.apache.spark.sql.execution.ProjectExec.$anonfun$doConsume$1(basicPhysicalOperators.scala:66)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\r\n\tat org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:66)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:194)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:149)\r\n\tat org.apache.spark.sql.execution.InputAdapter.consume(WholeStageCodegenExec.scala:496)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce(WholeStageCodegenExec.scala:483)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce$(WholeStageCodegenExec.scala:456)\r\n\tat org.apache.spark.sql.execution.InputAdapter.doProduce(WholeStageCodegenExec.scala:496)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\r\n\tat org.apache.spark.sql.execution.InputAdapter.produce(WholeStageCodegenExec.scala:496)\r\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:51)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\r\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:41)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:632)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:692)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\r\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:316)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:434)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:420)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3627)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2697)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2697)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2904)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:337)\r\n\tat sun.reflect.GeneratedMethodAccessor80.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.lang.IllegalArgumentException: All week-based patterns are unsupported since Spark 3.0, detected: Y, Please use the SQL function EXTRACT instead\r\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.$anonfun$convertIncompatiblePattern$4(DateTimeFormatterHelper.scala:323)\r\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.$anonfun$convertIncompatiblePattern$4$adapted(DateTimeFormatterHelper.scala:321)\r\n\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.immutable.StringOps.foreach(StringOps.scala:33)\r\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)\r\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.$anonfun$convertIncompatiblePattern$2(DateTimeFormatterHelper.scala:321)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\r\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.convertIncompatiblePattern(DateTimeFormatterHelper.scala:318)\r\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper.getOrCreateFormatter(DateTimeFormatterHelper.scala:121)\r\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper.getOrCreateFormatter$(DateTimeFormatterHelper.scala:117)\r\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.getOrCreateFormatter(TimestampFormatter.scala:59)\r\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.formatter$lzycompute(TimestampFormatter.scala:68)\r\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.formatter(TimestampFormatter.scala:67)\r\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.validatePatternString(TimestampFormatter.scala:108)\r\n\t... 75 more\r\n"
     ]
    }
   ],
   "source": [
    "df.select(date_format(df.Date,\"YYYYMM\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+---------+------+---+----------------+-----------------+----------+---------+----------+------------------+\n",
      "|Customer ID|     Name|  Surname|Gender|Age|          Region|JobClassification|DateJoined|  Balance|      Date|Avg Montly Balance|\n",
      "+-----------+---------+---------+------+---+----------------+-----------------+----------+---------+----------+------------------+\n",
      "|  100000001|    Simon|    Walsh|  Male| 21|         England|     White Collar| 05.Jan.15|113810.15|2021-01-01| 9484.179166666667|\n",
      "|  400000002|  Jasmine|   Miller|Female| 34|Northern Ireland|      Blue Collar| 06.Jan.15| 36919.73|2021-01-01| 3076.644166666667|\n",
      "|  100000003|     Liam|    Brown|  Male| 46|         England|     White Collar| 07.Jan.15|101536.83|2021-01-01|         8461.4025|\n",
      "|  300000004|   Trevor|     Parr|  Male| 32|           Wales|     White Collar| 08.Jan.15|  1421.52|2021-01-01|            118.46|\n",
      "|  100000005|  Deirdre|  Pullman|Female| 38|         England|      Blue Collar| 09.Jan.15| 35639.79|2021-01-01|         2969.9825|\n",
      "|  300000006|      Ava|  Coleman|Female| 30|           Wales|      Blue Collar| 09.Jan.15|122443.77|2021-01-01|10203.647500000001|\n",
      "|  100000007|  Dorothy|  Thomson|Female| 34|         England|      Blue Collar| 11.Jan.15| 42879.84|2021-01-01|3573.3199999999997|\n",
      "|  200000008|     Lisa|     Knox|Female| 48|        Scotland|            Other| 11.Jan.15| 36680.17|2021-01-01|3056.6808333333333|\n",
      "|  300000009|     Ruth| Campbell|Female| 33|           Wales|     White Collar| 11.Jan.15| 74284.35|2021-01-01|         6190.3625|\n",
      "|  100000010|  Dominic|     Parr|  Male| 42|         England|     White Collar| 12.Jan.15| 10912.45|2021-01-01| 909.3708333333334|\n",
      "|  100000011|  Dominic|    Lewis|  Male| 40|         England|     White Collar| 12.Jan.15| 39667.83|2021-01-01|         3305.6525|\n",
      "|  100000012| Benjamin|    Grant|  Male| 39|         England|     White Collar| 12.Jan.15| 32281.62|2021-01-01|2690.1349999999998|\n",
      "|  100000013|     Ryan|MacDonald|  Male| 24|         England|     White Collar| 12.Jan.15| 40781.63|2021-01-01|3398.4691666666663|\n",
      "|  200000014|   Thomas| Lawrence|  Male| 46|        Scotland|            Other| 12.Jan.15| 48791.46|2021-01-01|          4065.955|\n",
      "|  300000015|Madeleine| Marshall|Female| 36|           Wales|            Other| 12.Jan.15|  2846.03|2021-01-01|237.16916666666668|\n",
      "|  100000016| Nicholas|   Newman|  Male| 42|         England|     White Collar| 14.Jan.15|  2116.85|2021-01-01|176.40416666666667|\n",
      "|  200000017|    Grace|     Hill|Female| 31|        Scotland|            Other| 14.Jan.15| 10356.31|2021-01-01| 863.0258333333333|\n",
      "|  200000018| Samantha|  Coleman|Female| 42|        Scotland|            Other| 14.Jan.15|  3801.69|2021-01-01|          316.8075|\n",
      "|  100000019|  William|     Ince|  Male| 40|         England|      Blue Collar| 15.Jan.15| 65534.69|2021-01-01| 5461.224166666667|\n",
      "|  100000020|   Audrey|    Jones|Female| 46|         England|      Blue Collar| 15.Jan.15| 11462.64|2021-01-01| 955.2199999999999|\n",
      "+-----------+---------+---------+------+---+----------------+-----------------+----------+---------+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('Avg Montly Balance',df.Balance/12).show()  #Adding additional column in the view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+---------+------+---+--------+-----------------+----------+---------+----------+\n",
      "|Customer ID|   Name|  Surname|Gender|Age|  Region|JobClassification|DateJoined|  Balance|      Date|\n",
      "+-----------+-------+---------+------+---+--------+-----------------+----------+---------+----------+\n",
      "|  200000022|  Jason|   Butler|  Male| 58|Scotland|      Blue Collar| 18.Jan.15| 21252.97|2021-01-01|\n",
      "|  200000024|   Carl|    Quinn|  Male| 52|Scotland|      Blue Collar| 19.Jan.15|  6580.81|2021-01-01|\n",
      "|  200000026|Richard|   Fraser|  Male| 55|Scotland|      Blue Collar| 21.Jan.15| 43249.26|2021-01-01|\n",
      "|  200000033|  Grace|  Chapman|Female| 58|Scotland|            Other| 29.Jan.15|  25865.2|2021-01-01|\n",
      "|  200000040|  Gavin|  Pullman|  Male| 59|Scotland|            Other| 31.Jan.15|114456.23|2021-01-01|\n",
      "|  100000045|Dorothy| Johnston|Female| 53| England|            Other| 03.Feb.15|  2893.19|2021-01-01|\n",
      "|  200000068|  Lucas|    Piper|  Male| 61|Scotland|      Blue Collar| 12.Feb.15|  7162.43|2021-01-01|\n",
      "|  200000069|  Jason|    Sharp|  Male| 53|Scotland|      Blue Collar| 12.Feb.15| 35751.37|2021-01-01|\n",
      "|  200000072| Angela|  Roberts|Female| 51|Scotland|            Other| 12.Feb.15| 35937.38|2021-01-01|\n",
      "|  200000100|Carolyn|Robertson|Female| 56|Scotland|            Other| 16.Mar.15| 13251.27|2021-01-01|\n",
      "|  200000101|Stewart|    Lewis|  Male| 51|Scotland|      Blue Collar| 16.Mar.15|119014.87|2021-01-01|\n",
      "|  200000103| Nicola|   Powell|Female| 55|Scotland|     White Collar| 16.Mar.15| 35939.04|2021-01-01|\n",
      "|  200000104|  Diana| McDonald|Female| 57|Scotland|            Other| 16.Mar.15|  22686.0|2021-01-01|\n",
      "|  200000106|   Ella|     Bond|Female| 54|Scotland|            Other| 16.Mar.15| 44973.03|2021-01-01|\n",
      "|  200000109|    Joe|  Forsyth|  Male| 63|Scotland|            Other| 16.Mar.15| 73461.71|2021-01-01|\n",
      "|  200000110|    Tim|    Jones|  Male| 63|Scotland|      Blue Collar| 16.Mar.15| 26474.23|2021-01-01|\n",
      "|  200000118|   Adam|  McGrath|  Male| 52|Scotland|            Other| 28.Mar.15| 67682.92|2021-01-01|\n",
      "|  200000140|  Keith|   Harris|  Male| 57|Scotland|      Blue Collar| 02.Apr.15| 40510.36|2021-01-01|\n",
      "|  200000164| Claire|    Walsh|Female| 59|Scotland|            Other| 05.Apr.15| 67524.87|2021-01-01|\n",
      "|  200000165|   Paul|   Harris|  Male| 52|Scotland|      Blue Collar| 05.Apr.15|  4371.98|2021-01-01|\n",
      "+-----------+-------+---------+------+---+--------+-----------------+----------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(df.Age>50).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-------+------+---+--------+-----------------+----------+--------+----------+\n",
      "|Customer ID| Name|Surname|Gender|Age|  Region|JobClassification|DateJoined| Balance|      Date|\n",
      "+-----------+-----+-------+------+---+--------+-----------------+----------+--------+----------+\n",
      "|  200000022|Jason| Butler|  Male| 58|Scotland|      Blue Collar| 18.Jan.15|21252.97|2021-01-01|\n",
      "+-----------+-----+-------+------+---+--------+-----------------+----------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(\"Name=='Jason' and Surname='Butler'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+--------+------+---+----------------+-----------------+----------+--------+----------+\n",
      "|Customer ID| Name| Surname|Gender|Age|          Region|JobClassification|DateJoined| Balance|      Date|\n",
      "+-----------+-----+--------+------+---+----------------+-----------------+----------+--------+----------+\n",
      "|  200000022|Jason|  Butler|  Male| 58|        Scotland|      Blue Collar| 18.Jan.15|21252.97|2021-01-01|\n",
      "|  200000069|Jason|   Sharp|  Male| 53|        Scotland|      Blue Collar| 12.Feb.15|35751.37|2021-01-01|\n",
      "|  200000104|Diana|McDonald|Female| 57|        Scotland|            Other| 16.Mar.15| 22686.0|2021-01-01|\n",
      "|  100000161|Diana|  Powell|Female| 42|         England|     White Collar| 05.Apr.15|  2337.0|2021-01-01|\n",
      "|  200000191|Jason|  Fisher|  Male| 51|        Scotland|      Blue Collar| 08.Apr.15|  1239.6|2021-01-01|\n",
      "|  300000428|Diana|   Bower|Female| 49|           Wales|     White Collar| 09.May.15|20738.67|2021-01-01|\n",
      "|  200000504|Jason|   Lyman|  Male| 51|        Scotland|     White Collar| 15.May.15|16320.71|2021-01-01|\n",
      "|  300000592|Diana|  Martin|Female| 27|           Wales|     White Collar| 19.May.15|98541.75|2021-01-01|\n",
      "|  200000699|Jason| Cameron|  Male| 60|        Scotland|            Other| 27.May.15| 49081.5|2021-01-01|\n",
      "|  300000703|Diana|  Hodges|Female| 34|           Wales|     White Collar| 27.May.15|24796.57|2021-01-01|\n",
      "|  400000769|Jason|  Walker|  Male| 24|Northern Ireland|     White Collar| 31.May.15|78004.29|2021-01-01|\n",
      "|  200000803|Jason|   Clark|  Male| 49|        Scotland|      Blue Collar| 04.Jun.15|56825.53|2021-01-01|\n",
      "|  100000825|Jason|   Berry|  Male| 33|         England|            Other| 07.Jun.15|27818.08|2021-01-01|\n",
      "|  100000875|Jason| MacLeod|  Male| 47|         England|     White Collar| 14.Jun.15|69979.67|2021-01-01|\n",
      "|  400001022|Jason|   Bower|  Male| 38|Northern Ireland|     White Collar| 28.Jun.15|76999.31|2021-01-01|\n",
      "|  100001057|Diana|    Ross|Female| 26|         England|            Other| 02.Jul.15|10054.71|2021-01-01|\n",
      "|  100001397|Jason|   Piper|  Male| 36|         England|     White Collar| 27.Jul.15|23674.84|2021-01-01|\n",
      "|  100001416|Jason|    Nash|  Male| 44|         England|     White Collar| 28.Jul.15|23330.41|2021-01-01|\n",
      "|  100001713|Jason|McDonald|  Male| 34|         England|      Blue Collar| 23.Aug.15|21191.18|2021-01-01|\n",
      "|  300001750|Diana|   Piper|Female| 48|           Wales|     White Collar| 23.Aug.15|36947.59|2021-01-01|\n",
      "+-----------+-----+--------+------+---+----------------+-----------------+----------+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(\"Name in ('Jason','Diana')\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+--------+------+---+----------------+-----------------+----------+--------+----------+\n",
      "|Customer ID| Name| Surname|Gender|Age|          Region|JobClassification|DateJoined| Balance|      Date|\n",
      "+-----------+-----+--------+------+---+----------------+-----------------+----------+--------+----------+\n",
      "|  200000022|Jason|  Butler|  Male| 58|        Scotland|      Blue Collar| 18.Jan.15|21252.97|2021-01-01|\n",
      "|  200000069|Jason|   Sharp|  Male| 53|        Scotland|      Blue Collar| 12.Feb.15|35751.37|2021-01-01|\n",
      "|  200000104|Diana|McDonald|Female| 57|        Scotland|            Other| 16.Mar.15| 22686.0|2021-01-01|\n",
      "|  100000161|Diana|  Powell|Female| 42|         England|     White Collar| 05.Apr.15|  2337.0|2021-01-01|\n",
      "|  200000191|Jason|  Fisher|  Male| 51|        Scotland|      Blue Collar| 08.Apr.15|  1239.6|2021-01-01|\n",
      "|  300000428|Diana|   Bower|Female| 49|           Wales|     White Collar| 09.May.15|20738.67|2021-01-01|\n",
      "|  200000504|Jason|   Lyman|  Male| 51|        Scotland|     White Collar| 15.May.15|16320.71|2021-01-01|\n",
      "|  300000592|Diana|  Martin|Female| 27|           Wales|     White Collar| 19.May.15|98541.75|2021-01-01|\n",
      "|  200000699|Jason| Cameron|  Male| 60|        Scotland|            Other| 27.May.15| 49081.5|2021-01-01|\n",
      "|  300000703|Diana|  Hodges|Female| 34|           Wales|     White Collar| 27.May.15|24796.57|2021-01-01|\n",
      "|  400000769|Jason|  Walker|  Male| 24|Northern Ireland|     White Collar| 31.May.15|78004.29|2021-01-01|\n",
      "|  200000803|Jason|   Clark|  Male| 49|        Scotland|      Blue Collar| 04.Jun.15|56825.53|2021-01-01|\n",
      "|  100000825|Jason|   Berry|  Male| 33|         England|            Other| 07.Jun.15|27818.08|2021-01-01|\n",
      "|  100000875|Jason| MacLeod|  Male| 47|         England|     White Collar| 14.Jun.15|69979.67|2021-01-01|\n",
      "|  400001022|Jason|   Bower|  Male| 38|Northern Ireland|     White Collar| 28.Jun.15|76999.31|2021-01-01|\n",
      "|  100001057|Diana|    Ross|Female| 26|         England|            Other| 02.Jul.15|10054.71|2021-01-01|\n",
      "|  100001397|Jason|   Piper|  Male| 36|         England|     White Collar| 27.Jul.15|23674.84|2021-01-01|\n",
      "|  100001416|Jason|    Nash|  Male| 44|         England|     White Collar| 28.Jul.15|23330.41|2021-01-01|\n",
      "|  100001713|Jason|McDonald|  Male| 34|         England|      Blue Collar| 23.Aug.15|21191.18|2021-01-01|\n",
      "|  300001750|Diana|   Piper|Female| 48|           Wales|     White Collar| 23.Aug.15|36947.59|2021-01-01|\n",
      "+-----------+-----+--------+------+---+----------------+-----------------+----------+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(df.Name.isin ('Jason','Diana')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------------+\n",
      "|     Name|((Age >= 30) AND (Age <= 50))|\n",
      "+---------+-----------------------------+\n",
      "|    Simon|                        false|\n",
      "|  Jasmine|                         true|\n",
      "|     Liam|                         true|\n",
      "|   Trevor|                         true|\n",
      "|  Deirdre|                         true|\n",
      "|      Ava|                         true|\n",
      "|  Dorothy|                         true|\n",
      "|     Lisa|                         true|\n",
      "|     Ruth|                         true|\n",
      "|  Dominic|                         true|\n",
      "|  Dominic|                         true|\n",
      "| Benjamin|                         true|\n",
      "|     Ryan|                        false|\n",
      "|   Thomas|                         true|\n",
      "|Madeleine|                         true|\n",
      "| Nicholas|                         true|\n",
      "|    Grace|                         true|\n",
      "| Samantha|                         true|\n",
      "|  William|                         true|\n",
      "|   Audrey|                         true|\n",
      "+---------+-----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.Name,df.Age.between(30,50)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+--------+------+---+----------------+-----------------+----------+---------+----------+\n",
      "|Customer ID|     Name| Surname|Gender|Age|          Region|JobClassification|DateJoined|  Balance|      Date|\n",
      "+-----------+---------+--------+------+---+----------------+-----------------+----------+---------+----------+\n",
      "|  400000002|  Jasmine|  Miller|Female| 34|Northern Ireland|      Blue Collar| 06.Jan.15| 36919.73|2021-01-01|\n",
      "|  100000003|     Liam|   Brown|  Male| 46|         England|     White Collar| 07.Jan.15|101536.83|2021-01-01|\n",
      "|  300000004|   Trevor|    Parr|  Male| 32|           Wales|     White Collar| 08.Jan.15|  1421.52|2021-01-01|\n",
      "|  100000005|  Deirdre| Pullman|Female| 38|         England|      Blue Collar| 09.Jan.15| 35639.79|2021-01-01|\n",
      "|  300000006|      Ava| Coleman|Female| 30|           Wales|      Blue Collar| 09.Jan.15|122443.77|2021-01-01|\n",
      "|  100000007|  Dorothy| Thomson|Female| 34|         England|      Blue Collar| 11.Jan.15| 42879.84|2021-01-01|\n",
      "|  200000008|     Lisa|    Knox|Female| 48|        Scotland|            Other| 11.Jan.15| 36680.17|2021-01-01|\n",
      "|  300000009|     Ruth|Campbell|Female| 33|           Wales|     White Collar| 11.Jan.15| 74284.35|2021-01-01|\n",
      "|  100000010|  Dominic|    Parr|  Male| 42|         England|     White Collar| 12.Jan.15| 10912.45|2021-01-01|\n",
      "|  100000011|  Dominic|   Lewis|  Male| 40|         England|     White Collar| 12.Jan.15| 39667.83|2021-01-01|\n",
      "|  100000012| Benjamin|   Grant|  Male| 39|         England|     White Collar| 12.Jan.15| 32281.62|2021-01-01|\n",
      "|  200000014|   Thomas|Lawrence|  Male| 46|        Scotland|            Other| 12.Jan.15| 48791.46|2021-01-01|\n",
      "|  300000015|Madeleine|Marshall|Female| 36|           Wales|            Other| 12.Jan.15|  2846.03|2021-01-01|\n",
      "|  100000016| Nicholas|  Newman|  Male| 42|         England|     White Collar| 14.Jan.15|  2116.85|2021-01-01|\n",
      "|  200000017|    Grace|    Hill|Female| 31|        Scotland|            Other| 14.Jan.15| 10356.31|2021-01-01|\n",
      "|  200000018| Samantha| Coleman|Female| 42|        Scotland|            Other| 14.Jan.15|  3801.69|2021-01-01|\n",
      "|  100000019|  William|    Ince|  Male| 40|         England|      Blue Collar| 15.Jan.15| 65534.69|2021-01-01|\n",
      "|  100000020|   Audrey|   Jones|Female| 46|         England|      Blue Collar| 15.Jan.15| 11462.64|2021-01-01|\n",
      "|  300000021|    Boris|Johnston|  Male| 37|           Wales|            Other| 16.Jan.15|  31778.9|2021-01-01|\n",
      "|  300000023|  Deirdre|McDonald|Female| 41|           Wales|     White Collar| 18.Jan.15| 66785.78|2021-01-01|\n",
      "+-----------+---------+--------+------+---+----------------+-----------------+----------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(df.Age.between(30,50)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#desc , #isnull , #like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "keyword can't be an expression (<ipython-input-177-20732a89b686>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-177-20732a89b686>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    join(df.Name=df1.Name).show()\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m keyword can't be an expression\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3224"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.join(df2,df.CustomerID==df2.CustomerID,how='inner').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4014"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.join(df2,df.CustomerID==df2.CustomerID,how='outer').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4014"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.join(df2,df.CustomerID==df2.CustomerID,how='left').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3224"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.join(df2,df.CustomerID==df2.CustomerID,how='right').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3224"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.join(df2,df.CustomerID==df2.CustomerID,how='left_semi').count()       #Inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "790"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.join(df2,df.CustomerID==df2.CustomerID,how='left_anti').count()     #Table1-Table2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+--------+------------+\n",
      "|     Name|max(CustomerID)|max(Age)|max(Balance)|\n",
      "+---------+---------------+--------+------------+\n",
      "| Samantha|      400000706|      63|     86123.0|\n",
      "|  Carolyn|      400003848|      61|   118676.95|\n",
      "|      Sue|      400000488|      59|   139784.01|\n",
      "|    Scott|      100003978|      35|     18055.9|\n",
      "|    Grace|      400001001|      58|   138086.22|\n",
      "|    Lucas|      300003104|      61|   105718.04|\n",
      "|    Keith|      400003743|      61|   112243.76|\n",
      "|   Alison|      400002618|      64|   143808.39|\n",
      "|   Andrea|      400000676|      59|   136370.38|\n",
      "|   Amanda|      400003415|      57|    94722.42|\n",
      "|     Mary|      300002188|      55|    99806.54|\n",
      "|    James|      400002075|      56|   120278.47|\n",
      "|   Trevor|      400000830|      58|    99642.13|\n",
      "| Benjamin|      300004000|      57|   144209.77|\n",
      "|    Faith|      300003655|      51|   143879.62|\n",
      "|    Jason|      400001022|      61|   125213.72|\n",
      "|   Nathan|      300002689|      57|   126150.99|\n",
      "|  Natalie|      400002716|      53|    149684.4|\n",
      "|Christian|      400001297|      59|   115312.87|\n",
      "| Jennifer|      400002638|      47|   121591.95|\n",
      "+---------+---------------+--------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Name').max().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+------------------+------------------+\n",
      "|     Name|     avg(CustomerID)|          avg(Age)|      avg(Balance)|\n",
      "+---------+--------------------+------------------+------------------+\n",
      "| Samantha|1.5454743677272728E8| 37.54545454545455|23635.302272727276|\n",
      "|  Carolyn|1.7272910577272728E8| 40.63636363636363| 37982.76136363637|\n",
      "|      Sue|2.0000236188235295E8|36.294117647058826| 42331.39352941177|\n",
      "|    Scott|        1.00003978E8|              35.0|           18055.9|\n",
      "|    Grace|1.4230950792307693E8| 37.73076923076923| 38909.44307692308|\n",
      "|    Lucas| 1.666684574722222E8|41.638888888888886|33797.195555555554|\n",
      "|    Keith|1.7931248179310346E8| 39.62068965517241| 38144.08586206898|\n",
      "|   Alison| 1.777796548888889E8| 36.18518518518518| 43162.58851851852|\n",
      "|   Andrea| 1.545477055909091E8| 40.40909090909091|58807.494090909095|\n",
      "|   Amanda|1.8214487214285713E8|             35.25| 41665.08964285714|\n",
      "|     Mary|     1.50002373875E8|            37.625| 41534.60541666666|\n",
      "|    James| 2.096791936451613E8| 36.38709677419355| 38974.18451612903|\n",
      "|   Trevor| 1.821447199642857E8|38.035714285714285| 39813.59642857142|\n",
      "| Benjamin|       1.653864505E8| 40.15384615384615| 38327.70115384616|\n",
      "|    Faith|1.2631817315789473E8| 34.31578947368421| 46283.48105263158|\n",
      "|    Jason| 1.690497620952381E8|40.976190476190474| 46789.52023809523|\n",
      "|   Nathan| 1.535734857857143E8| 38.92857142857143|40975.657857142854|\n",
      "|  Natalie| 1.833351857222222E8| 33.94444444444444|46319.833333333336|\n",
      "|Christian|      1.7200188404E8|             41.88| 42966.45440000002|\n",
      "| Jennifer|2.2222435755555555E8| 35.22222222222222| 47005.32666666667|\n",
      "+---------+--------------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Name').avg().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "|     Name|      sum(Balance)|\n",
      "+---------+------------------+\n",
      "| Samantha| 519976.6500000001|\n",
      "|  Carolyn| 835620.7500000001|\n",
      "|      Sue| 719633.6900000001|\n",
      "|    Scott|           18055.9|\n",
      "|    Grace|1011645.5200000001|\n",
      "|    Lucas|        1216699.04|\n",
      "|    Keith|1106178.4900000005|\n",
      "|   Alison|        1165389.89|\n",
      "|   Andrea|        1293764.87|\n",
      "|   Amanda|        1166622.51|\n",
      "|     Mary| 996830.5299999998|\n",
      "|    James|        1208199.72|\n",
      "|   Trevor|1114780.6999999997|\n",
      "| Benjamin| 996520.2300000001|\n",
      "|    Faith|         879386.14|\n",
      "|    Jason|1965159.8499999999|\n",
      "|   Nathan|        1147318.42|\n",
      "|  Natalie|          833757.0|\n",
      "|Christian|1074161.3600000003|\n",
      "| Jennifer|         846095.88|\n",
      "+---------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Name').agg({'Balance':'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|     Name|max(Age)|\n",
      "+---------+--------+\n",
      "| Samantha|      63|\n",
      "|  Carolyn|      61|\n",
      "|      Sue|      59|\n",
      "|    Scott|      35|\n",
      "|    Grace|      58|\n",
      "|    Lucas|      61|\n",
      "|    Keith|      61|\n",
      "|   Alison|      64|\n",
      "|   Andrea|      59|\n",
      "|   Amanda|      57|\n",
      "|     Mary|      55|\n",
      "|    James|      56|\n",
      "|   Trevor|      58|\n",
      "| Benjamin|      57|\n",
      "|    Faith|      51|\n",
      "|    Jason|      61|\n",
      "|   Nathan|      57|\n",
      "|  Natalie|      53|\n",
      "|Christian|      59|\n",
      "| Jennifer|      47|\n",
      "+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Name').agg({'Age':'max'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|        sum(Balance)|\n",
      "+--------------------+\n",
      "|1.5962252337000012E8|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg({'Balance':'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|     Name|count|\n",
      "+---------+-----+\n",
      "| Samantha|   22|\n",
      "|  Carolyn|   22|\n",
      "|      Sue|   17|\n",
      "|    Scott|    1|\n",
      "|    Grace|   26|\n",
      "|    Lucas|   36|\n",
      "|    Keith|   29|\n",
      "|   Alison|   27|\n",
      "|   Andrea|   22|\n",
      "|   Amanda|   28|\n",
      "|     Mary|   24|\n",
      "|    James|   31|\n",
      "|   Trevor|   28|\n",
      "| Benjamin|   26|\n",
      "|    Faith|   19|\n",
      "|    Jason|   42|\n",
      "|   Nathan|   28|\n",
      "|  Natalie|   18|\n",
      "|Christian|   25|\n",
      "| Jennifer|   18|\n",
      "+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Name').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
